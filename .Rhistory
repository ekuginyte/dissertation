# Refit the model using the optimal lambda value obtained from cross-validation
set.seed(7)
model_fit <- glmnet(x = X, y = y, alpha = alpha, lambda = model_cv$lambda.min)
# Extract the coefficients from the  model
coefficients <- coef(model_fit, s = model_fit$lambda.min)
# Find the names of the variables with non-zero coefficients
selected_variable_names <- rownames(coefficients)[coefficients[, 1] != 0]
# Extract the non-zero coefficients
selected_predictors <- coefficients[selected_variable_names, 1] %>% data.frame()
# Return the list of selected predictors and model itself
return(list(selected_predictors = selected_predictors, model_fit = lasso_model))
}
# Loop over prefixes and suffixes
for (prefix in prefixes) {
for (suffix in suffixes) {
# Construct the dataset name
dataset_name <- paste(prefix, suffix, sep = "_")
# Access the dataset from the global environment
dataset <- get(dataset_name)
# Fit the model for each alpha value (0.5 and 1)
for (alpha in c(0.5, 1)) {
# Fit the model
model <- fit_glmnet(dataset, alpha = alpha)
# Generate a model name
model_name <- paste(prefix, suffix, ifelse(alpha == 0.5, "elnet", "lasso"), sep = "_")
# Store the model in the list
models_list[[model_name]] <- model
}
}
}
#         data - a data frame containing the predictors and the response variable.
#                The response variable should be named "y".
#         nfolds - number of folds for cross-validation, default = 10.
#         alpha - numeric entry 1 for Lasso, 0.5 for Elastic Net.
# OUTPUT:
#         A list containing:
#               selected_predictors - a data frame with the predictors selected by the model
#                                     with their respective coefficients.
#               model_fit - the fitted glmnet model.
#
fit_glmnet <- function(data, nfolds = 10, alpha = 0.5) {
# Input checks
# Ensure data is a data.frame
if (!is.data.frame(data)) {
stop("Input 'data' must be a data frame.")
}
# Ensure 'y' is in the data
if (!"y" %in% names(data)) {
stop("The response variable 'y' is not present in the input data.")
}
# Ensure 'nfolds' is a positive integer
if (!is.numeric(nfolds) || nfolds <= 0 || round(nfolds) != nfolds) {
stop("'nfolds' must be a positive integer.")
}
# Ensure only Lasso or Elnet alpha values are fit
if (!is.numeric(alpha) || !(alpha %in% c(0.5, 1))) {
stop("Alpha should be a numeric value of either 0.5 (Elastic Net) or 1 (Lasso).")
}
# Extract the target
y <- data$y
# Remove the original categorical variables
data <- data.frame(subset(data, select = -y))
# Combine intercept, scaled continuous variables
X <- model.matrix(~ . - 1, data = data)
# Perform k-fold cross-validation to find the optimal value of the regularisation
#   parameter lambda that minimises the cross-validation error
set.seed(7)
model_cv <- cv.glmnet(x = X, y = y, alpha = alpha, nfolds = nfolds)
# Plot the cross-validation errors as a function of lambda.
plot(model_cv)
abline(v = log(model_cv$lambda.min), lwd = 4, lty = 2)
# Refit the model using the optimal lambda value obtained from cross-validation
set.seed(7)
model_fit <- glmnet(x = X, y = y, alpha = alpha, lambda = model_cv$lambda.min)
# Extract the coefficients from the  model
coefficients <- coef(model_fit, s = model_fit$lambda.min)
# Find the names of the variables with non-zero coefficients
selected_variable_names <- rownames(coefficients)[coefficients[, 1] != 0]
# Extract the non-zero coefficients
selected_predictors <- coefficients[selected_variable_names, 1] %>% data.frame()
# Return the list of selected predictors and model itself
return(list(selected_predictors = selected_predictors, model_fit = lasso_model))
}
#         data - a data frame containing the predictors and the response variable.
#                The response variable should be named "y".
#         nfolds - number of folds for cross-validation, default = 10.
#         alpha - numeric entry 1 for Lasso, 0.5 for Elastic Net.
# OUTPUT:
#         A list containing:
#               selected_predictors - a data frame with the predictors selected by the model
#                                     with their respective coefficients.
#               model_fit - the fitted glmnet model.
#
fit_glmnet <- function(data, nfolds = 10, alpha = 0.5) {
# Input checks
# Ensure data is a data.frame
if (!is.data.frame(data)) {
stop("Input 'data' must be a data frame.")
}
# Ensure 'y' is in the data
if (!"y" %in% names(data)) {
stop("The response variable 'y' is not present in the input data.")
}
# Ensure 'nfolds' is a positive integer
if (!is.numeric(nfolds) || nfolds <= 0 || round(nfolds) != nfolds) {
stop("'nfolds' must be a positive integer.")
}
# Ensure only Lasso or Elnet alpha values are fit
if (!is.numeric(alpha) || !(alpha %in% c(0.5, 1))) {
stop("Alpha should be a numeric value of either 0.5 (Elastic Net) or 1 (Lasso).")
}
# Extract the target
y <- data$y
# Remove the original categorical variables
data <- data.frame(subset(data, select = -y))
# Combine intercept, scaled continuous variables
X <- model.matrix(~ . - 1, data = data)
# Perform k-fold cross-validation to find the optimal value of the regularisation
#   parameter lambda that minimises the cross-validation error
set.seed(7)
model_cv <- cv.glmnet(x = X, y = y, alpha = alpha, nfolds = nfolds)
# Plot the cross-validation errors as a function of lambda.
plot(model_cv)
abline(v = log(model_cv$lambda.min), lwd = 4, lty = 2)
# Refit the model using the optimal lambda value obtained from cross-validation
set.seed(7)
model_fit <- glmnet(x = X, y = y, alpha = alpha, lambda = model_cv$lambda.min)
# Extract the coefficients from the  model
coefficients <- coef(model_fit, s = model_fit$lambda.min)
# Find the names of the variables with non-zero coefficients
selected_variable_names <- rownames(coefficients)[coefficients[, 1] != 0]
# Extract the non-zero coefficients
selected_predictors <- coefficients[selected_variable_names, 1] %>% data.frame()
# Return the list of selected predictors and model itself
return(list(selected_predictors = selected_predictors, model_fit = model_fit))
}
# Loop over prefixes and suffixes
for (prefix in prefixes) {
for (suffix in suffixes) {
# Construct the dataset name
dataset_name <- paste(prefix, suffix, sep = "_")
# Access the dataset from the global environment
dataset <- get(dataset_name)
# Fit the model for each alpha value (0.5 and 1)
for (alpha in c(0.5, 1)) {
# Fit the model
model <- fit_glmnet(dataset, alpha = alpha)
# Generate a model name
model_name <- paste(prefix, suffix, ifelse(alpha == 0.5, "elnet", "lasso"), sep = "_")
# Store the model in the list
models_list[[model_name]] <- model
}
}
}
seq(1, 20, 1)
# Combine the list of libraries from both scripts
library_list <- c("tidyverse", "corrplot", "betareg", "R.matlab", "glmnet", "dplyr",
"cowplot", "coda", "car", "igraph", "R6", "nimble", "MASS", "xgboost",
"caret", "spikeslab", "SSLASSO", "horseshoe", "bayesreg", "Hmisc",
"LaplacesDemon", "BayesS5", "monomvn", "Hmisc", "gridExtra", "maps")
# Un-comment the following lines if you need to install the packages
# for (i in library_list) {
#   install.packages(i, character.only = TRUE)
# }
# Load the libraries
for (i in library_list) {
library(i, character.only = TRUE)
}
# Set working directory (assuming you want to set it to the 'main' directory)
setwd("~/Documents/Dissertation/main/Dissertation")
# Remove unwanted objects
rm(library_list, i)
source("simulate_data.R")
source("functions.R")
source("data_crime_raw.R")
rm(df_st)
# Run the LASSO function and extract the selected coefficients
crime_lasso <- fit_glmnet(data = df_t, alpha = 1)
saveRDS(crime_lasso, "~/Documents/Dissertation/main/dissertation/results/crime/crime_lasso.rds")
#### CRIME. ELNET PENALISED REGRESSION 'glmnet' ####
# Run the elnet function and extract the selected coefficients
crime_elnet <- fit_glmnet(data = df_t, alpha = 0.5)
saveRDS(crime_elnet, "~/Documents/Dissertation/main/dissertation/results/crime/crime_elnet.rds")
crime_lasso
View(crime_elnet)
View(crime_lasso)
crime_elnet[["selected_predictors"]][["."]]
crime_elnet$selected_predictors
# Fit spikeslab model
crime_spikeslab_prior <- fit_spikeslab_prior(data = df_t, bigp_smalln = FALSE)
# Fit spikeslab model
crime_spikeslab_prior <- fit_spikeslab_prior(data = df_t, bigp_smalln = FALSE)
saveRDS(crime_spikeslab_prior, "~/Documents/Dissertation/main/dissertation/results/crime/crime_spikeslab_prior.rds")
crime_spikeslab_prior$spikeslab.obj
#### CRIME. SPIKE AND SLAB LASSO 'SSLASSO' ####
# Call the function with the Crimes data
crime_ssl <- fit_sslasso(data = df_t, var = "fixed")
saveRDS(crime_ssl, "~/Documents/Dissertation/main/dissertation/results/crime/crime_ssl.rds")
#sslasso_crime
# Run the functions an extract the results
# Truncated Cauchy prior
crime_horseshoe_tc <- fit_hs_horseshoe(data = df_t, method.tau = "truncatedCauchy",
method.sigma = "Jeffreys", burn = 5000,
nmc = 10000, thin = 1, alpha = 0.05)
saveRDS(crime_horseshoe_tc, "~/Documents/Dissertation/main/dissertation/results/crime/crime_horseshoe_tc.rds")
crime_horseshoe_tc$sel_var
# Half Cauchy prior
crime_horseshoe_hc <- fit_hs_horseshoe(data = df_t, method.tau = "halfCauchy",
method.sigma = "Jeffreys",
burn = 5000, nmc = 10000,
thin = 1, alpha = 0.05)
saveRDS(crime_horseshoe_hc, "~/Documents/Dissertation/main/dissertation/results/crime/crime_horseshoe_hc.rds")
crime_horseshoe_hc$sel_var
rm(hist_t_list)
rm(scatter_t_list)
rm(hist_list)
# Fit the model
hs_bs_crime <- fit_horseshoe_bs(data = df_t, prior = "hs")
saveRDS(hs_bs_crime, "~/Documents/Dissertation/main/dissertation/results/crime/hs_bs_crime.rds")
#### CRIME. HORSESHOE + PRIOR 'bayesreg' ####
# Fit the model
hsp_bs_crime <- fit_horseshoe_bs(data = df_t, prior = "hs+")
saveRDS(hs_bsp_crime, "~/Documents/Dissertation/main/dissertation/results/crime/hs_bsp_crime.rds")
hsp_bs_crime
saveRDS(hs_bsp_crime, "~/Documents/Dissertation/main/dissertation/results/crime/hs_bsp_crime.rds")
saveRDS(hsp_bs_crime, "~/Documents/Dissertation/main/dissertation/results/crime/hsp_bs_crime.rds")
hs_bs_crime$selected_variables
hsp_bs_crime$selected_variables
# Fit the model
blasso_crime <- fit_blasso(data = df_t)
View(crime_elnet)
crime_elnet$selected_predictors
crime_elnet$selected_predictors$.
crime_elnet_sel_pred <- data.frame(crime_elnet$selected_predictors)
View(crime_elnet_sel_pred)
crime_elnet_sel_pred$. %>% as.vector()
crime_elnet_sel_pred$. %>% as.vector()
rownames(crime_elnet_sel_pred)
crime_elnet_sel_pred$. %>% as.vector()
crime_lassod$. %>% as.vector()
crime_lasso$. %>% as.vector()
crime_lasso$selected_predictors
crime_lasso$selected_predictors$.
rownames(crime_lasso)
rownames(crime_lasso$selected_predictors)
crime_spikeslab_prior
crime_spikeslab_prior$spikeslab.obj
crime_spikeslab_prior
#     data               - A data frame containing the predictors and the response variable.
#                          The response variable should be named "y".
#     bigp_smalln        - A logical indicating if the high-dimensional low sample size adjustments
#                          should be made. Should be either TRUE or FALSE.
#     bigp_smalln_factor - A numeric adjustment factor to be used when bigp.smalln is TRUE.
#     seed               - An NEGATIVE integer used for setting the seed for reproducibility.
#
# OUTPUT:
#     ss_results - The fitted Spike and Slab model.
#
fit_spikeslab_prior <- function(data, bigp_smalln, bigp_smalln_factor = 0,
screen = FALSE, K = 10, seed = -42) {
# Input validation
if (!is.data.frame(data)) {
stop("data should be a data frame.")
}
if (!"y" %in% names(data)) {
stop("data should contain a column named 'y' as the response variable.")
}
if (!is.logical(bigp_smalln) || length(bigp_smalln) != 1) {
stop("bigp_smalln should be a logical value (either TRUE or FALSE).")
}
if (!is.numeric(bigp_smalln_factor) || length(bigp_smalln_factor) != 1) {
stop("bigp_smalln_factor should be a single numeric value.")
}
if (!is.logical(screen) || length(screen) != 1) {
stop("screen should be a logical value (either TRUE or FALSE).")
}
if (!is.numeric(seed) || length(seed) != 1 || seed > 0 || seed != as.integer(seed)) {
stop("seed should be a single negative integer value.")
}
# Extract the target
y <- scale(data$y)
# Remove the 'y' column from the data frame and convert the remaining data into a model matrix
X <- model.matrix(~ . - 1, data = data[, -which(names(data) == "y")])
# Scale the predictor variables
X <- scale(X)
# Run the spikeslab model
ss_results <- spikeslab::cv.spikeslab(
# Formula representing the relationship between predictors and response
#formula,
# The matrix containing the variables in the formula
x = X,
y = y,
K = K,
# The number of iterations in the two MCMC chains used in spikeslab.
# n.iter1 is for the first chain, and n.iter2 is for the second chain.
n.iter1 = 1000,
n.iter2 = 5000,
# Calculate the mean squared error as part of the model evaluation
mse = TRUE,
# High-dimensional low sample size adjustments.
# bigp.smalln - logical flag, if TRUE adjustments for high-dimensional low sample size are made.
# bigp.smalln.factor - controls the magnitude of the adjustments.
bigp.smalln = bigp_smalln,
bigp.smalln.factor = bigp_smalln_factor,
# To screen the variables when p is big
screen = screen,
# Random effects. If specified, adds random effects to the model
r.effects = NULL,
# Maximum number of variables to be retained in the model
max.var = 500,
# If TRUE, the predictors are centered by subtracting their means
center = TRUE,
# If TRUE, an intercept term is included in the model
intercept = FALSE,
# If TRUE, a fast approximation algorithm is used for speed up
fast = TRUE,
# The number of blocks into which the coefficients are split for Gibbs sampling
beta.blocks = 5,
# If TRUE, outputs progress and additional information while fitting the model
verbose = FALSE,
# The number of trees in the ensemble (used if using Bayesian Additive Regression Trees prior)
ntree = 300,
# Seed for random number generator, for reproducibility of results
seed = seed,
intercept = FALSE
)
# Plot the path of the estimates for the Spike and Slab model
plot(ss_results, plot.type = "path")
# Return the result
return(ss_results)
}
crime_ssl
crime_ssl$selected_variable_names
# The output contains coefficients, ever_selected, and plot
crime_ssl$coefficients
crime_ssl$ever_selected
crime_ssl$selected_variable_names
# Call the function with the Crimes data
crime_ssl <- fit_sslasso(data = df_t, var = "unknown")
crime_ssl$selected_variable_names
# Fit spikeslab model
crime_spikeslab_prior <- fit_spikeslab_prior(data = df_st, bigp_smalln = FALSE)
df_st <- scale(df_t)
# Fit spikeslab model
crime_spikeslab_prior <- fit_spikeslab_prior(data = df_st, bigp_smalln = FALSE)
df_st <- scale(df_t) %>% data.frame()
# Fit spikeslab model
crime_spikeslab_prior <- fit_spikeslab_prior(data = df_st, bigp_smalln = FALSE)
#     data               - A data frame containing the predictors and the response variable.
#                          The response variable should be named "y".
#     bigp_smalln        - A logical indicating if the high-dimensional low sample size adjustments
#                          should be made. Should be either TRUE or FALSE.
#     bigp_smalln_factor - A numeric adjustment factor to be used when bigp.smalln is TRUE.
#     seed               - An NEGATIVE integer used for setting the seed for reproducibility.
#
# OUTPUT:
#     ss_results - The fitted Spike and Slab model.
#
fit_spikeslab_prior <- function(data, bigp_smalln, bigp_smalln_factor = 0,
screen = FALSE, K = 10, seed = -42) {
# Input validation
if (!is.data.frame(data)) {
stop("data should be a data frame.")
}
if (!"y" %in% names(data)) {
stop("data should contain a column named 'y' as the response variable.")
}
if (!is.logical(bigp_smalln) || length(bigp_smalln) != 1) {
stop("bigp_smalln should be a logical value (either TRUE or FALSE).")
}
if (!is.numeric(bigp_smalln_factor) || length(bigp_smalln_factor) != 1) {
stop("bigp_smalln_factor should be a single numeric value.")
}
if (!is.logical(screen) || length(screen) != 1) {
stop("screen should be a logical value (either TRUE or FALSE).")
}
if (!is.numeric(seed) || length(seed) != 1 || seed > 0 || seed != as.integer(seed)) {
stop("seed should be a single negative integer value.")
}
# Extract the target
y <- scale(data$y)
# Remove the 'y' column from the data frame and convert the remaining data into a model matrix
X <- model.matrix(~ . - 1, data = data[, -which(names(data) == "y")])
# Scale the predictor variables
X <- scale(X)
# Run the spikeslab model
ss_results <- spikeslab::cv.spikeslab(
# Formula representing the relationship between predictors and response
#formula,
# The matrix containing the variables in the formula
x = X,
y = y,
K = K,
# The number of iterations in the two MCMC chains used in spikeslab.
# n.iter1 is for the first chain, and n.iter2 is for the second chain.
n.iter1 = 1000,
n.iter2 = 5000,
# Calculate the mean squared error as part of the model evaluation
mse = TRUE,
# High-dimensional low sample size adjustments.
# bigp.smalln - logical flag, if TRUE adjustments for high-dimensional low sample size are made.
# bigp.smalln.factor - controls the magnitude of the adjustments.
bigp.smalln = bigp_smalln,
bigp.smalln.factor = bigp_smalln_factor,
# To screen the variables when p is big
screen = screen,
# Random effects. If specified, adds random effects to the model
r.effects = NULL,
# Maximum number of variables to be retained in the model
max.var = 500,
# If TRUE, the predictors are centered by subtracting their means
center = TRUE,
# If TRUE, an intercept term is included in the model
intercept = FALSE,
# If TRUE, a fast approximation algorithm is used for speed up
fast = TRUE,
# The number of blocks into which the coefficients are split for Gibbs sampling
beta.blocks = 5,
# If TRUE, outputs progress and additional information while fitting the model
verbose = FALSE,
# The number of trees in the ensemble (used if using Bayesian Additive Regression Trees prior)
ntree = 300,
# Seed for random number generator, for reproducibility of results
seed = seed
)
# Plot the path of the estimates for the Spike and Slab model
plot(ss_results, plot.type = "path")
# Return the result
return(ss_results)
}
# Fit spikeslab model
crime_spikeslab_prior <- fit_spikeslab_prior(data = df_st, bigp_smalln = FALSE)
crime_spikeslab_prior$spikeslab.obj
crime_ssl$selected_variable_names
#     theta - Prior mixing proportion.
#     eps - Convergence criterion.
#     plot_width - Width of the plot in inches.
#     plot_height - Height of the plot in inches.
# OUTPUTS:
#     A list containing:
#         coefficients - The fitted matrix of coefficients.
#         ever_selected - A binary vector indicating which variables were
#                        ever selected along the regularization path.
#         plot - A plot of the coefficient paths for the fitted model.
fit_sslasso <- function(data, lambda1 = 1, lambda0 = seq(1, nrow(data), length.out = 100),
var = "fixed", plot_width = 6, plot_height = 4) {
# Input checks
# Check that 'data' is a data frame
if (!is.data.frame(data)) {
stop("'data' must be a data frame.")
}
# Check that 'lambda1' is a positive numeric value
if (!is.numeric(lambda1) || lambda1 <= 0) {
stop("'lambda1' must be a positive numeric value.")
}
# Check that 'lambda0' is a numeric sequence
if (!is.numeric(lambda0)) {
stop("'lambda0' must be a numeric sequence.")
}
# Check that 'plot_width' is a positive numeric value
if (!is.numeric(plot_width) || plot_width <= 0) {
stop("'plot_width' must be a positive numeric value.")
}
# Check that 'plot_height' is a positive numeric value
if (!is.numeric(plot_height) || plot_height <= 0) {
stop("'plot_height' must be a positive numeric value.")
}
# Extract the target
y <- data$y
# Remove the original categorical variables
data <- data.frame(subset(data, select = -y))
# Combine scaled continuous variables, intercept
X <- model.matrix(~ . - 1, data = data)
# Fit the SSLASSO model
set.seed(42)
result <- SSLASSO(X = X, y = y, penalty = "adaptive", variance = var,
lambda1 = lambda1, lambda0 = lambda0, warn = TRUE)
# Set plot margins (bottom, left, top, right)
par(mar = c(6, 6, 2, 2))
# Set the plot dimensions (width, height) in inches
par(pin = c(plot_width, plot_height))
# Create the plot of coefficients
plot(result)
# Extract selection indicators and determine which variables were ever selected
selected_variables <- result$select
ever_selected <- apply(selected_variables, 1, max)
# Assuming X is a data.frame or matrix
variable_names <- colnames(X)
# selected_variable_indices gives us the indices of the selected variables
selected_variable_indices <- which(ever_selected == 1)
# Get the names of the selected variables
selected_variable_names <- variable_names[selected_variable_indices]
# Return the results as a list
return(list(coefficients = result$beta, ever_selected = ever_selected,
selected_variable_names = selected_variable_names, plot = result))
}
crime_ssl$selected_variable_names
crime_ssl$ever_selected
crime_horseshoe_tc
crime_horseshoe_tc$sel_var
crime_horseshoe_hc$sel_var
crime_horseshoe_tc$sel_var
crime_horseshoe_tc$sel_var
View(crime_horseshoe_tc)
crime_horseshoe_hc$sel_var
crime_horseshoe_hc$sel_var
# Fit the model
hsp_bs_crime <- fit_horseshoe_bs(data = df_t, prior = "hs+")
hsp_bs_crime$selected_variables
hsp_bs_crime$selected_variables
hsp_bs_crime$selected_variables
rm(hsp_bs_crime, hs_bs_crime)
