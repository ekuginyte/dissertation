startsWith(Sector, "09") ~ "Recreation and culture",
startsWith(Sector, "10") ~ "Education",
startsWith(Sector, "11") ~ "Restaurants and hotels",
startsWith(Sector, "12") ~ "Miscellaneous goods and services",
TRUE ~ "Other"  # Default case
))
CPI_data1 <- CPI_data1[!is.na(CPI_data1$Sector), ]
# Perform Kruskal-Wallis test
result_CPI <- kruskal.test(Value9 ~ Title, data = CPI_data1)
# Print the results
print(result_CPI)
### CORRELATION BETWEEN 2022 and 2023 ###
# Loop over all columns excluding the first and last one
CPI_data1[, -c(1, ncol(CPI_data1))] <- lapply(CPI_data1[, -c(1, ncol(CPI_data1))], function(x) as.numeric(as.character(x)))
# Correlation matrix for the full data
correlation_matrix <- cor(CPI_data1[, -c(1, ncol(CPI_data1))])
# Print correlation matrix
print(correlation_matrix)
# Generate correlation plot
corrplot(correlation_matrix, method="color", type="upper",
addCoef.col = "black") # Add correlation coefficients to plot
# Plot
ggplot(std_dev_data_long, aes(x = Month, y = SD, color = Title)) +
geom_point() +
labs(title = "Standard Deviation over Time by Title", x = "Month", y = "Standard Deviation") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(std_dev_data_long, aes(x = Month, y = SD, group = Title)) +
geom_line() +
facet_wrap(~ Title, scales = "free_y") +
theme_minimal() +
labs(title = "Standard Deviation over Time by Title", x = "Month", y = "Standard Deviation")
# Create the scatter plot
ggplot(sector_analysis, aes(x = Mean_Indirect_Energy_Intensity, y = Mean_CPI, color = EnCat2)) +
geom_point() +
labs(x = "Mean Indirect Energy Intensity", y = "Mean CPI") +
theme_minimal()
# Plot the data
ggplot(sector_analysis, aes(x = Energy, y = `2022`, color = EnCat)) +
geom_point() +
labs(x = "Energy Intensity", y = "CPI") +
theme_minimal()
library(tidyverse)
library(readr)
census2021_ts001_oa <- read_csv("Downloads/full data/census2021-ts001-oa.csv")
View(census2021_ts001_oa)
### LOAD DATA ###
df1 <- read_csv("Downloads/full data/census2021-ts001-oa.csv")
### LOAD DATA ###
df1 <- read_csv("Downloads/full data/census2021-ts001-oa.csv")
summary(df1)
files <- list.files(path = "Downloads/full data/", pattern = "*.csv", full.names = TRUE)
### LOAD DATA ###
# List all files to upload together
files <- list.files(path = "Downloads/full data/", pattern = "*.csv", full.names = TRUE)
## Read and merge the files
# Start with the first file
merged_df <- read_csv(files[1])
# Loop all files to merge them into one data frame by reference "geography"
for (file in files[-1]) {  # Loop over the rest of the files
df <- read_csv(file)
merged_df <- merge(merged_df, df, by = "geography")
}
rm(df1)
View(df)
# Loop all files to merge them into one data frame by reference "geography"
for (file in files[-1]) {  # Loop over the rest of the files
df <- read_csv(file)
merged_df <- merge(merged_df, df, by = "geography")
}
View(merged_df)
# Remove the duplicate columns
# Find column names that contain "date" or "geography code"
cols_to_remove <- grep("date|geography code", names(merged_df), ignore.case = TRUE, value = TRUE)
# Remove these columns
merged_df <- merged_df[ , !(names(merged_df) %in% cols_to_remove)]
View(merged_df)
# Identify which columns have identical values
# Initialize an empty vector to hold the names of identical columns
identical_cols <- c()
# Loop through each column of the dataframe
for (col_name in names(merged_df)) {
# If the number of unique values in a column is 1, add it to the vector
if (length(unique(merged_df[[col_name]])) == 1) {
identical_cols <- c(identical_cols, col_name)
}
}
print(identical_cols)
# Identify which columns have identical values
# Transpose the data frame
transposed_df <- as.data.frame(t(merged_df))
# Initialize an empty vector to hold the names of identical columns
identical_rows <- c()
# Loop through each row of the transposed dataframe (which corresponds to the columns of the original dataframe)
for (row_name in rownames(transposed_df)) {
# If the number of unique values in a row is 1, add it to the vector
if (length(unique(transposed_df[row_name, , drop = FALSE])) == 1) {
identical_rows <- c(identical_rows, row_name)
}
}
# Identify which columns have identical values
# Create a new dataframe containing only columns with "Total" in their names
total_df <- merged_df[ , grepl("Total", names(merged_df))]
View(total_df)
View(merged_df)
summary(total_df)
# Explore just the marital status
marital <- read_csv("Downloads/full data/census2021-ts002-oa.csv")
View(marital)
summary(maritcal)
summary(marital)
sum(marital$`Marital and civil partnership status: Married or in a registered civil partnership: Married: Opposite sex; measures: Value`, marital$`Marital and civil partnership status: Married or in a registered civil partnership: Married: Same sex; measures: Value`) == sum(marital$`Marital and civil partnership status: Married or in a registered civil partnership: Married; measures: Value`)
marital$check_equal <- apply(marital, 1, function(row) {
sum_values <- row["Marital and civil partnership status: Married or in a registered civil partnership: Married: Opposite sex; measures: Value"] +
row["Marital and civil partnership status: Married or in a registered civil partnership: Married: Same sex; measures: Value"]
total_value <- row["Marital and civil partnership status: Married or in a registered civil partnership: Married; measures: Value"]
return(abs(sum_values - total_value) < 1e-6)  # comparing floating point numbers for equality
})
marital$rel_vs_norel <- marital$`Marital and civil partnership status: Divorced or civil partnership dissolved; measures: Value` /
marital$`Marital and civil partnership status: Married or in a registered civil partnership; measures: Value`
marital$commited_vs_single <- marital$`Marital and civil partnership status: Married or in a registered civil partnership; measures: Value` /
marital$`Marital and civil partnership status: Never married and never registered a civil partnership; measures: Value`
merged_df$divorce_rate <- marital$`Marital and civil partnership status: Divorced or civil partnership dissolved; measures: Value` /
merged_df$`Residence type: Total; measures: Value`
merged_df$divorce_rate <- marital$`Marital and civil partnership status: Divorced or civil partnership dissolved; measures: Value` /
merged_df$`Age: Total.x`
merged_df <- arrange(merged_df, desc(divorce_rate))
first_row <- merged_df[1, ]
first_row <- transpose(first_row)
View(first_row)
marital$`Marital and civil partnership status: Divorced or civil partnership dissolved; measures: Value` /
# Install the package if you haven't already
install.packages("openxlsx")
# Load the package
library(openxlsx)
# Write the data frame to an Excel file
write.xlsx(merged_df, "merged_df.xlsx")
checkshite <- merged_df$`Marital and civil partnership status: Divorced or civil partnership dissolved: Divorced; measures: Value.x` /
merged_df$`Age: Total.y`
checkshite <- as.data.frame(merged_df$`Marital and civil partnership status: Divorced or civil partnership dissolved: Divorced; measures: Value.x`,
merged_df$`Age: Total.y`)
View(checkshite)
checkshite <- as.data.frame(merged_df$`Marital and civil partnership status: Divorced or civil partnership dissolved: Divorced; measures: Value.x`,
merged_df$`Age: Total.x`)
merged_df$`Age: Total.x`
checkshite <- as.data.frame(merged_df$`Marital and civil partnership status: Divorced or civil partnership dissolved: Divorced; measures: Value.x`,
merged_df$`Age: Total.x`)
checkshite <- data.frame(merged_df$`Marital and civil partnership status: Divorced or civil partnership dissolved: Divorced; measures: Value.x`,
merged_df$`Age: Total.x`)
View(checkshite)
checkshite <- data.frame(
Divorced = merged_df$`Marital and civil partnership status: Divorced or civil partnership dissolved: Divorced; measures: Value.x`,
TotalAge = merged_df$`Age: Total.x`
)
rows <- which(checksite$Divorced > checksite$TotalAge)
rows <- which(checkshite$Divorced > checkshite$TotalAge)
rows
merged_df$divorce_rate <-
checkshite <- data.frame(merged_df$`Marital and civil partnership status: Divorced or civil partnership dissolved: Divorced; measures: Value.x`,
merged_df$`Age: Total.x`)
View(checkshite)
merged_df$divorce_rate <-
checkshite <- data.frame(
Divorced = merged_df$`Marital and civil partnership status: Divorced or civil partnership dissolved: Divorced; measures: Value.x`,
TotalAge = merged_df$`Age: Total.x`
)
checkshite$Prop <- checkshite$Divorced / checkshite$TotalAge
View(total_df)
total_df <- dplyr::select(total_df, -Age.total.y)
total_df <- dplyr::select(total_df, -Age:Total.y)
total_df <- dplyr::select(total_df, -"Age:Total.y")
total_df <- dplyr::select(total_df, -"Age: Total.y")
install.packages("R.matlab")
library(R.matlab)
mydata <- readMat('PersonGaitDataSet.mat')
mydata <- readMat('~/Downloads/PersonGaitDataSet.mat')
View(mydata)
summary(mydata)
summary(mydata$X)
summary(mydata$Y)
X_df <- as.data.frame(mydata$X)
Y_df <- as.data.frame(mydata$Y)
mydata_df <- as.data.frame(cbind(mydata$X, mydata$Y))
View(mydata_df)
library(readxl)
Residential_Building_Data_Set <- read_excel("Downloads/Residential-Building-Data-Set.xlsx")
View(Residential_Building_Data_Set)
# Example usage
# Simulate data with p = 20 covariates
set.seed(42)
data <- simulate_linear_regression(20)
simulate_linear_regression <- function(p) {
# Set sample size
n <- 30
# Set error variance
sigma_epsilon <- sqrt(1/2)
# Generate covariates
X <- matrix(rnorm(n * p), nrow = n)
# Set regression coefficients
beta <- numeric(p)
beta[1:2] <- 5
beta[3:5] <- 1
# beta[6:p-5] <- 0 (already zeros by default)
# Generate errors
epsilon <- rnorm(n, mean = 0, sd = sigma_epsilon)
# Generate response vector
Y <- X %*% beta + epsilon
# Convert X to a data frame
data <- as.data.frame(X)
# Add the response vector Y to the data frame
data$Y <- Y
# Return the generated data frame
return(data)
}
# Example usage
# Simulate data with p = 20 covariates
set.seed(42)
data <- simulate_linear_regression(20)
View(data)
# Function to simulate a data set
# INPUT:
#       n - number of data points to simulate
#       seed - seed for random number generation
#       v_strong - vector of weights for strongly correlated variables
#       v_weak - vector of weights for weakly correlated variables
# OUTPUT:
#       sim_data - data frame with simulated data
simulate_data <- function(n = 30,               # Number of observations
p_independent = 200,  # Number of independent covariates
p_collinear = 60,     # Number of collinear covariates
include_polynomials = FALSE,
include_interactions = FALSE,
tau = 2,              # Precision of normal distribution for error terms
seed = NULL) {        # Seed for random number generation
# Set seed if provided
if (!is.null(seed)) {
set.seed(seed)
}
# Generate independent regressors
x_independent <- matrix(rnorm(n * p_independent, mean = 0, sd = 1), ncol = p_independent)
# Generate collinear regressors
x_collinear <- matrix(rnorm(n * p_collinear, mean = 0, sd = 1), nrow = n, ncol = p_collinear)
# Create collinear features based on original collinear features
x1x2 <- (x_collinear[, 1] + x_collinear[, 2]) / 2
x1x5 <- x_collinear[, 1] + rnorm(n, mean = 0, sd = 1 / sqrt(tau))
x1xJ <- (x_collinear[, 1] + x_collinear[, p_collinear]) / 2
x_collinear <- cbind(x_collinear, x1x2, x1x5, x1xJ)
# Combine independent and collinear regressors
x <- cbind(x_independent, x_collinear)
# If include_polynomials is TRUE, generate polynomial features
if (include_polynomials) {
x_poly <- x ^ 2
x <- cbind(x, x_poly)
}
# If include_interactions is TRUE, generate interaction terms
if (include_interactions) {
interaction_term <- x[, 1] * x[, 2]
x <- cbind(x, interaction_term)
}
# Generate coefficients
p_total <- ncol(x)
beta <- c(rep(5, 5), rep(1, 5), rep(0, p_total - 10))
# Set intercept
alpha <- 0
# Generate error terms
eps <- rnorm(n, mean = 0, sd = 1 / sqrt(tau))
# Generate response variable
y <- alpha + as.vector(x %*% beta + eps)
# Return the synthetic data as a list
return(list(x = x, y = y))
}
# Example usage with seed for reproducibility:
df_1 <- simulate_data(n = 30,
p_independent = 10,
p_collinear = 10,
include_polynomials = TRUE,
include_interactions = TRUE,
seed = 42)
View(df_1)
# Extract x and y from the returned list
x <- synthetic_data$x
# Extract x and y from the returned list
x <- df_1$x
y <- df_1$y
View(x)
# Function to simulate a data set
# INPUT:
#       n - number of data points to simulate
#       seed - seed for random number generation
#       v_strong - vector of weights for strongly correlated variables
#       v_weak - vector of weights for weakly correlated variables
# OUTPUT:
#       sim_data - data frame with simulated data
simulate_data <- function(n = 30,               # Number of observations
p_independent = 200,  # Number of independent covariates
p_collinear = 60,     # Number of collinear covariates
include_polynomials = FALSE,
include_interactions = FALSE,
tau = 2,              # Precision of normal distribution for error terms
seed = NULL) {        # Seed for random number generation
# Set seed if provided
if (!is.null(seed)) {
set.seed(seed)
}
# Generate independent regressors
x_independent <- matrix(rnorm(n * p_independent, mean = 0, sd = 1), ncol = p_independent)
# Generate collinear regressors
x_collinear <- matrix(rnorm(n * p_collinear, mean = 0, sd = 1), nrow = n, ncol = p_collinear)
# Create collinear features based on original collinear features
x1x2 <- (x_collinear[, 1] + x_collinear[, 2]) / 2
x1x5 <- x_collinear[, 1] + rnorm(n, mean = 0, sd = 1 / sqrt(tau))
x1xJ <- (x_collinear[, 1] + x_collinear[, p_collinear]) / 2
x_collinear <- cbind(x_collinear, x1x2, x1x5, x1xJ)
# Combine independent and collinear regressors
x <- cbind(x_independent, x_collinear)
# If include_polynomials is TRUE, generate polynomial features
if (include_polynomials) {
x_poly <- x ^ 2
x <- cbind(x, x_poly)
}
# If include_interactions is TRUE, generate interaction terms
if (include_interactions) {
interaction_term <- x[, 1] * x[, 2]
x <- cbind(x, interaction_term)
}
# Generate coefficients
p_total <- ncol(x)
beta <- c(rep(5, 5), rep(1, 5), rep(0, p_total - 10))
# Set intercept
alpha <- 0
# Generate error terms
eps <- rnorm(n, mean = 0, sd = 1 / sqrt(tau))
# Generate response variable
y <- alpha + as.vector(x %*% beta + eps)
# Return the synthetic data as a list
return(data.frame(x, y = y))
}
# Example usage with seed for reproducibility:
df_1 <- simulate_data(n = 30,
p_independent = 10,
p_collinear = 10,
include_polynomials = TRUE,
include_interactions = TRUE,
seed = 42)
View(df_1)
correlation_matrix <- corr(df_1[, 46])
correlation_matrix <- cor(df_1[, 46])
# Calculate correlation matrix
correlation_matrix <- cor(df_1)
# Print the correlation matrix
print(correlation_matrix)
heatmap(correlation_matrix)
corrplot(correlation_matrix)
### SET UP ###
# Packages/libraries required to install
library_list <- c("tidyverse", "corrplot", "betareg", "dplyr", "cowplot",
"coda", "igraph", "R6", "nimble")
# Load the libraries
for (i in library_list) {
library(i, character.only = TRUE)
}
corrplot(correlation_matrix)
cor(df_1)
# Combine the list of libraries from both scripts
library_list <- c("tidyverse", "corrplot", "betareg", "R.matlab", "glmnet", "dplyr",
"cowplot", "coda", "igraph", "R6", "nimble", "MASS", "xgboost",
"caret", "spikeslab", "SSLASSO", "horseshoe", "bayesreg", "Hmisc",
"LaplacesDemon", "BayesS5", "monomvn", "Hmisc")
# Load the libraries
for (i in library_list) {
library(i, character.only = TRUE)
}
# Set working directory (assuming you want to set it to the 'main' directory)
setwd("~/Documents/Dissertation/main/Dissertation")
# Remove unwanted objects
rm(library_list, i)
# Source the file that contains the simulation functions
source("simulate_data.R")
# Function to fit Lasso regression on different datasets and extract the selected predictors
# INPUTS:
#         data - a data frame containing the predictors and the response variable.
#               The response variable should be named "y".
#         nfolds - number of folds for cross-validation (default is 10).
# OUTPUT:
#         selected_predictors - selected predictor data frame with coefficients.
#
fit_lasso <- function(data, nfolds = 10) {
# Input checks
# Ensure data is a data.frame
if (!is.data.frame(data)) {
stop("Input 'data' must be a data frame.")
}
# Ensure 'nfolds' is a function
if (!is.numeric(nfolds) || nfolds <= 0 || round(nfolds) != nfolds) {
stop("'nfolds' must be a positive integer.")
}
# Create the matrix of predictors by excluding the intercept term,
#   and standardise the predictors. Add an intercept column with all values equal to 1.
xmatrix <- cbind(Intercept = 1, scale(model.matrix(~ . - 1, data = data[, -1])))
# Fit the Lasso model on the design matrix with alpha = 1 (Lasso penalty)
initial_lasso <- glmnet(x = xmatrix, y = data$y, alpha = 1)
# Perform k-fold cross-validation to find the optimal value of the regularisation
#   parameter lambda that minimizes the cross-validation error
lasso_cv <- cv.glmnet(x = xmatrix, y = data$y, alpha = 1, nfolds = nfolds)
# Plot the regularisation path which is the coefficient profiles of the Lasso model
#   as a function of lambda. The vertical line represents the optimal lambda value
#   that minimizes the cross-validation error.
#par(mfrow = c(1, 2))
#plot(initial_lasso, xvar = "lambda")
#abline(v = log(initial_lasso$lambda.min), lwd = 4, lty = 2)
# Plot the cross-validation errors as a function of lambda.
plot(lasso_cv)
abline(v = log(lasso_cv$lambda.min), lwd = 4, lty = 2)
# Refit the Lasso model using the optimal lambda value obtained from cross-validation
lasso_model <- glmnet(x = xmatrix, y = data$y, alpha = 1, lambda = lasso_cv$lambda.min)
# Extract the coefficients from the lasso model
coefficients <- coef(lasso_model, s = lasso_cv$lambda.min)
# Find the names of the variables with non-zero coefficients
selected_variable_names <- rownames(coefficients)[coefficients[, 1] != 0]
# Extract the non-zero coefficients
selected_predictors <- coefficients[selected_variable_names, 1] %>% data.frame()
# Return the selected predictors
return(selected_predictors)
}
# Function to fit Elastic net regression on different datasets and extract the best-fitted model.
# INPUTS:
#         data - a data frame containing the predictors and the response variable.
#               The response variable should be named "y".
#         nfolds - number of folds for cross-validation (default is 10).
# OUTPUT:
#         selected_predictors - selected predictor data frame with coefficients.
#
fit_elnet <- function(data, nfolds = 10) {
# Input checks
# Ensure data is a data.frame
if (!is.data.frame(data)) {
stop("Input 'data' must be a data frame.")
}
# Ensure 'nfolds' is a function
if (!is.numeric(nfolds) || nfolds <= 0 || round(nfolds) != nfolds) {
stop("'nfolds' must be a positive integer.")
}
# Create the matrix of predictors by excluding the intercept term,
# and standardise the predictors. Add an intercept column with all values equal to 1.
xmatrix <- cbind(Intercept = 1, scale(model.matrix(~ . - 1, data = data[, -1])))
# Fit the Elastic net model on the design matrix with alpha = 0.5 (L1 + L2 penalties)
initial_elnet <- glmnet(x = xmatrix, y = data$y, alpha = 0.5)
# Perform k-fold cross-validation to find the optimal value of the regularization
# parameter lambda that minimizes the cross-validation error.
elnet_cv <- cv.glmnet(x = xmatrix, y = data$y, alpha = 0.5, nfolds = nfolds)
# Plot the regularisation path which is the coefficient profiles of the Elastic net model
# as a function of lambda. The vertical line represents the optimal lambda value
# that minimises the cross-validation error.
#par(mfrow = c(1, 2))
#plot(initial_elnet, xvar = "lambda")
#abline(v = log(initial_elnet$lambda.min), lwd = 4, lty = 2)
# Plot the cross-validation errors as a function of lambda.
plot(elnet_cv)
abline(v = log(elnet_cv$lambda.min), lwd = 4, lty = 2)
# Refit the Elastic net model using the optimal lambda value obtained from cross-validation.
elnet_model <- glmnet(x = xmatrix, y = data$y, alpha = 1, lambda = elnet_cv$lambda.min)
# Extract the coefficients from the lasso model
coefficients <- coef(elnet_model, s = elnet_cv$lambda.min)
# Find the names of the variables with non-zero coefficients
selected_variable_names <- rownames(coefficients)[coefficients[, 1] != 0]
# Extract the non-zero coefficients
selected_predictors <- coefficients[selected_variable_names, 1] %>% data.frame()
# Return the selected predictors
return(selected_predictors)
}
# List of dataset prefixes
prefixes <- c("T1", "T2", "T3", "T4")
# List of dataset suffixes
suffixes <- c("LD", "ED", "HD", "VD")
# List of methods ("_lasso" or "_elnet")
methods <- c("_lasso", "_elnet")
# Loop through prefixes, suffixes and methods
for (prefix in prefixes) {
for (suffix in suffixes) {
# construct the data variable name
data_var_name <- paste0(prefix, "_", suffix)
data_var <- get(data_var_name)
# construct the model variable names for lasso and elastic net
for (method in methods) {
model_var_name <- paste0(prefix, "_", suffix, method)
if (method == "_lasso") {
assign(model_var_name, fit_lasso(data_var))
} else if (method == "_elnet") {
assign(model_var_name, fit_elnet(data_var))
}
}
}
}
View(T2_LD_lasso)
View(T2_ED_lasso)
View(T2_HD_lasso)
View(T2_VD_lasso)
View(T2_LD_elnet)
View(T2_ED_elnet)
View(T2_HD_elnet)
View(T2_VD_elnet)
View(T3_LD_lasso)
View(T3_ED_lasso)
View(T3_HD_lasso)
View(T3_VD_lasso)
View(T3_LD_elnet)
View(T3_HD_elnet)
View(T3_ED_elnet)
View(T3_VD_elnet)
# Generating true regression coefficients beta (I am using random beta here)
beta <- c(rep(6, 5), rep(4, 5), rep(3, 5), rep(0, p - 15))
View(T4_ED)
