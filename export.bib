@book{Kahneman2011,
   author = {Daniel Kahneman},
   city = {New York, NY, US},
   publisher = {Farrar, Straus and Giroux},
   title = {Thinking, fast and slow},
   year = {2011},
}
@article{Dakalbab2022,
   abstract = {The security of a community is its topmost priority; hence, governments must take proper actions to reduce the crime rate. Consequently, the application of artificial intelligence (AI) in crime prediction is a significant and well-researched area. This study investigates AI strategies in crime prediction. We conduct a systematic literature review (SLR). Our review evaluates the models from numerous points of view, including the crime analysis type, crimes studied, prediction technique, performance metrics and evaluations, strengths and weaknesses of the proposed method, and limitations and future directions. We review 120 research papers published between 2008 and 2021 that cover AI approaches for crime prediction. We provide 34 crime categories researched by researchers and 23 distinct crime analysis methodologies after analyzing the selected research articles. On the other hand, we identify 64 different machine learning (ML) techniques for crime prediction. In addition, we observe that the most applied approach in crime prediction is the supervised learning approach. Furthermore, we discuss the evaluation and performance metrics, as well as the tools utilized in building the models and their strengths and weaknesses. Crime prediction AI techniques are a promising field of study, and there are several ML models that researchers have applied. Consequently, based upon this review, we provide advice and guidance for researchers working in this area of study.},
   author = {Fatima Dakalbab and Manar Abu Talib and Omnia Abu Waraga and Ali Bou Nassif and Sohail Abbas and Qassim Nasir},
   doi = {10.1016/j.ssaho.2022.100342},
   issn = {25902911},
   issue = {1},
   journal = {Social Sciences & Humanities Open},
   pages = {100342},
   publisher = {Elsevier BV},
   title = {Artificial intelligence & crime prediction: A systematic literature review},
   volume = {6},
   year = {2022},
}
@report{Bergstra2012,
   abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success-they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
   author = {James Bergstra and James Bergstra@umontreal Ca and Yoshua Bengio@umontreal Ca},
   journal = {Journal of Machine Learning Research},
   keywords = {deep learning,global optimization,model selection,neural networks,response surface modeling},
   pages = {281-305},
   title = {Random Search for Hyper-Parameter Optimization Yoshua Bengio},
   volume = {13},
   url = {http://scikit-learn.sourceforge.net.},
   year = {2012},
}
@article{Hasan2020,
   abstract = {Diabetes, also known as chronic illness, is a group of metabolic diseases due to a high level of sugar in the blood over a long period. The risk factor and severity of diabetes can be reduced significantly if the precise early prediction is possible. The robust and accurate prediction of diabetes is highly challenging due to the limited number of labeled data and also the presence of outliers (or missing values) in the diabetes datasets. In this literature, we are proposing a robust framework for diabetes prediction where the outlier rejection, filling the missing values, data standardization, feature selection, K-fold cross-validation, and different Machine Learning (ML) classifiers (k-nearest Neighbour, Decision Trees, Random Forest, AdaBoost, Naive Bayes, and XGBoost) and Multilayer Perceptron (MLP) were employed. The weighted ensembling of different ML models is also proposed, in this literature, to improve the prediction of diabetes where the weights are estimated from the corresponding Area Under ROC Curve (AUC) of the ML model. AUC is chosen as the performance metric, which is then maximized during hyperparameter tuning using the grid search technique. All the experiments, in this literature, were conducted under the same experimental conditions using the Pima Indian Diabetes Dataset. From all the extensive experiments, our proposed ensembling classifier is the best performing classifier with the sensitivity, specificity, false omission rate, diagnostic odds ratio, and AUC as 0.789, 0.934, 0.092, 66.234, and 0.950 respectively which outperforms the state-of-the-art results by 2.00 % in AUC. Our proposed framework for the diabetes prediction outperforms the other methods discussed in the article. It can also provide better results on the same dataset which can lead to better performance in diabetes prediction. Our source code for diabetes prediction is made publicly available.},
   author = {Md Kamrul Hasan and Md Ashraful Alam and Dola Das and Eklas Hossain and Mahmudul Hasan},
   doi = {10.1109/ACCESS.2020.2989857},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Diabetes prediction,Pima Indian Diabetic dataset,ensembling classifier,machine learning,missing values and outliers,multilayer perceptron},
   pages = {76516-76531},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Diabetes prediction using ensembling of different machine learning classifiers},
   volume = {8},
   year = {2020},
}
@article{Shin2015,
   abstract = {Bayesian model selection procedures based on nonlocal alternative prior densities are extended to ultrahigh dimensional settings and compared to other variable selection procedures using precision-recall curves. Variable selection procedures included in these comparisons include methods based on $g$-priors, reciprocal lasso, adaptive lasso, scad, and minimax concave penalty criteria. The use of precision-recall curves eliminates the sensitivity of our conclusions to the choice of tuning parameters. We find that Bayesian variable selection procedures based on nonlocal priors are competitive to all other procedures in a range of simulation scenarios, and we subsequently explain this favorable performance through a theoretical examination of their consistency properties. When certain regularity conditions apply, we demonstrate that the nonlocal procedures are consistent for linear models even when the number of covariates $p$ increases sub-exponentially with the sample size $n$. A model selection procedure based on Zellner's $g$-prior is also found to be competitive with penalized likelihood methods in identifying the true model, but the posterior distribution on the model space induced by this method is much more dispersed than the posterior distribution induced on the model space by the nonlocal prior methods. We investigate the asymptotic form of the marginal likelihood based on the nonlocal priors and show that it attains a unique term that cannot be derived from the other Bayesian model selection procedures. We also propose a scalable and efficient algorithm called Simplified Shotgun Stochastic Search with Screening (S5) to explore the enormous model space, and we show that S5 dramatically reduces the computing time without losing the capacity to search the interesting region in the model space. The S5 algorithm is available in an \verb R ~package \{\it BayesS5\} on \texttt\{CRAN\}.},
   author = {Minsuk Shin and Anirban Bhattacharya and Valen E. Johnson},
   month = {7},
   title = {Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in Ultrahigh-Dimensional Settings},
   url = {http://arxiv.org/abs/1507.07106},
   year = {2015},
}
@article{Hans2007,
   abstract = {Model search in regression with very large numbers of candidate predictors raises challenges for both model specification and computation, for which standard approaches such as Markov chain Monte Carlo (MCMC) methods are often infeasible or ineffective. We describe a novel shotgun stochastic search (SSS) approach that explores "interesting" regions of the resulting high-dimensional model spaces and quickly identifies regions of high posterior probability over models. We describe algorithmic and modeling aspects, priors over the model space that induce sparsity and parsimony over and above the traditional dimension penalization implicit in Bayesian and likelihood analyses, and parallel computation using cluster computers. We discuss an example from gene expression cancer genomics, comparisons with MCMC and other methods, and theoretical and simulation-based aspects of performance characteristics in large-scale regression model searches. We also provide software implementing the methods. © 2007 American Statistical Association.},
   author = {Chris Hans and Adrian Dobra and Mike West},
   doi = {10.1198/016214507000000121},
   issn = {01621459},
   issue = {478},
   journal = {Journal of the American Statistical Association},
   keywords = {Model averaging,Parallel computing,Regression model uncertainty,Stochastic search,Variable selection},
   month = {6},
   pages = {507-516},
   title = {Shotgun stochastic search for "large p" regression},
   volume = {102},
   year = {2007},
}
@article{Wang2019,
   abstract = {With the rapid development of the Internet, big data has been applied in a large amount of application. However, there are often redundant or irrelevant features in high dimensional data, so feature selection is particularly important. Because the feature subset obtained by a single feature selection method may be biased, an ensemble feature selection method named SA-EFS based on sort aggregation is proposed in this paper, and this method is oriented to classification tasks. For high-dimensional data sets, the results of three feature selection methods, chi-square test, maximum information coefficient and XGBoost, are aggregated by specific strategy. The integration effects of arithmetic mean and geometric mean aggregation strategy on this model are analyzed. In order to evaluate the classification and prediction performance of feature subset, three classifiers with excellent performance, KNN, Random Forest and XGBoost, are tested respectively, and the influence of threshold on classification performance is analyzed. The experimental results show that compared with the single feature selection method, the arithmetic mean aggregation ensemble feature selection can effectively improve the classification accuracy, and the threshold interval setting of 0.1 is a better choice.},
   author = {Jie Wang and Jing Xu and Chengan Zhao and Yan Peng and Hongpeng Wang},
   doi = {10.1080/21642583.2019.1620658},
   issn = {21642583},
   issue = {2},
   journal = {Systems Science and Control Engineering},
   keywords = {Ensemble feature selection,XGBoost,high-dimensional datasets,maximum information coefficient,sort aggregation},
   month = {11},
   pages = {32-39},
   publisher = {Taylor and Francis Ltd.},
   title = {An ensemble feature selection method for high-dimensional data based on sort aggregation},
   volume = {7},
   year = {2019},
}
@article{Chen2016,
   abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
   author = {Tianqi Chen and Carlos Guestrin},
   doi = {10.1145/2939672.2939785},
   month = {3},
   title = {XGBoost: A Scalable Tree Boosting System},
   url = {http://arxiv.org/abs/1603.02754 http://dx.doi.org/10.1145/2939672.2939785},
   year = {2016},
}
@article{,
   author = {Himel Mallick Nengjun Yi},
   doi = {10.4172/2155-6180.S1-005},
   issn = {21556180},
   journal = {Journal of Biometrics & Biostatistics},
   title = {Bayesian Methods for High Dimensional Linear Models},
   year = {2013},
}
@article{,
   author = {Himel Mallick Nengjun Yi},
   doi = {10.4172/2155-6180.S1-005},
   issn = {21556180},
   journal = {Journal of Biometrics & Biostatistics},
   title = {Bayesian Methods for High Dimensional Linear Models},
   year = {2013},
}
@article{Casella2009,
   abstract = {It has long been known that for the comparison of pairwise nested models, a decision based on the Bayes factor produces a consistent model selector (in the frequentist sense). Here we go beyond the usual consistency for nested pairwise models, and show that for a wide class of prior distributions, including intrinsic priors, the corresponding Bayesian procedure for variable selection in normal regression is consistent in the entire class of normal linear models. We find that the asymptotics of the Bayes factors for intrinsic priors are equivalent to those of the Schwarz (BIC) criterion. Also, recall that the Jeffreys-Lindley paradox refers to the well-known fact that a point null hypothesis on the normal mean parameter is always accepted when the variance of the conjugate prior goes to infinity. This implies that some limiting forms of proper prior distributions are not necessarily suitable for testing problems. Intrinsic priors are limits of proper prior distributions, and for finite sample sizes they have been proved to behave extremely well for variable selection in regression; a consequence of our results is that for intrinsic priors Lindley's paradox does not arise.},
   author = {George Casella and F. Javier Girón and M. Lina Martinez and Elias Moreno},
   doi = {10.1214/08-AOS606},
   issn = {00905364},
   issue = {3},
   journal = {Annals of Statistics},
   keywords = {Bayes factors,Consistency,Intrinsic priors,Linear models},
   month = {6},
   pages = {1207-1228},
   title = {Consistency of bayesian procedures for variable selection},
   volume = {37},
   year = {2009},
}
@book{Friston2007,
   abstract = {First edition. In an age where the amount of data collected from brain imaging is increasing constantly, it is of critical importance to analyse those data within an accepted framework to ensure proper integration and comparison of the information collected. This book describes the ideas and procedures that underlie the analysis of signals produced by the brain. The aim is to understand how the brain works, in terms of its functional architecture and dynamics. This book provides the background and methodology for the analysis of all types of brain imaging data, from functional magnetic resonance imaging to magnetoencephalography. Critically, Statistical Parametric Mapping provides a widely accepted conceptual framework which allows treatment of all these different modalities. This rests on an understanding of the brain's functional anatomy and the way that measured signals are caused experimentally. The book takes the reader from the basic concepts underlying the analysis of neuroimaging data to cutting edge approaches that would be difficult to find in any other source. Critically, the material is presented in an incremental way so that the reader can understand the precedents for each new development. This book will be particularly useful to neuroscientists engaged in any form of brain mapping; who have to contend with the real-world problems of data analysis and understanding the techniques they are using. It is primarily a scientific treatment and a didactic introduction to the analysis of brain imaging data. It can be used as both a textbook for students and scientists starting to use the techniques, as well as a reference for practicing neuroscientists. The book also serves as a companion to the software packages that have been developed for brain imaging data analysis. * An essential reference and companion for users of the SPM software * Provides a complete description of the concepts and procedures entailed by the analysis of brain images * Offers full didactic treatment of the basic mathematics behind the analysis of brain imaging data * Stands as a compendium of all the advances in neuroimaging data analysis over the past decade * Adopts an easy to understand and incremental approach that takes the reader from basic statistics to state of the art approaches such as Variational Bayes * Structured treatment of data analysis issues that links different modalities and models * Includes a series of appendices and tutorial-style chapters that makes even the most sophisticated approaches accessible. INTRODUCTION -- A short history of SPM. -- Statistical parametric mapping. -- Modelling brain responses. -- SECTION 1: COMPUTATIONAL ANATOMY -- Rigid-body Registration. -- Nonlinear Registration. -- Segmentation. -- Voxel-based Morphometry. -- SECTION 2: GENERAL LINEAR MODELS -- The General Linear Model. -- Contrasts & Classical Inference. -- Covariance Components. -- Hierarchical models. -- Random Effects Analysis. -- Analysis of variance. -- Convolution models for fMRI. -- Efficient Experimental Design for fMRI. -- Hierarchical models for EEG/MEG. -- SECTION 3: CLASSICAL INFERENCE -- Parametric procedures for imaging. -- Random Field Theory & inference. -- Topological Inference. -- False discovery rate procedures. -- Non-parametric procedures. -- SECTION 4: BAYESIAN INFERENCE -- Empirical Bayes & hierarchical models. -- Posterior probability maps. -- Variational Bayes. -- Spatiotemporal models for fMRI. -- Spatiotemporal models for EEG. -- SECTION 5: BIOPHYSICAL MODELS -- Forward models for fMRI. -- Forward models for EEG and MEG. -- Bayesian inversion of EEG models. -- Bayesian inversion for induced responses. -- Neuronal models of ensemble dynamics. -- Neuronal models of energetics. -- Neuronal models of EEG and MEG. -- Bayesian inversion of dynamic models -- Bayesian model selection & averaging. -- SECTION 6: CONNECTIVITY -- Functional integration. -- Functional Connectivity. -- Effective Connectivity. -- Nonlinear coupling and Kernels. -- Multivariate autoregressive models. -- Dynamic Causal Models for fMRI. -- Dynamic Causal Models for EEG. -- Dynamic Causal Models & Bayesian selection. -- APPENDICES -- Linear models and inference. -- Dynamical systems. -- Expectation maximisation. -- Variational Bayes under the Laplace approximation. -- Kalman Filtering. -- Random Field Theory.},
   author = {K. J. (Karl J.) Friston and John Ashburner and Stefan Kiebel and Thomas Nichols and William D. Penny},
   isbn = {9780123725608},
   pages = {647},
   publisher = {Elsevier/Academic Press},
   title = {Statistical parametric mapping : the analysis of funtional brain images},
   year = {2007},
}
@report{Mackay1998,
   abstract = {Maximum a posteriori optimization of parameters and the Laplace approximation for the marginal likelihood are both basis-dependent methods. This note compares two choices of basis for models parameterized by probabilities, showing that it is possible to improve on the traditional choice, the probability simplex, by transforming to the 'softmax' basis.},
   author = {David J C Mackay},
   journal = {Machine Learning},
   keywords = {Bayes factor,Bayesian inference,graphical models,hidden Markov models,latent variable models,marginal likelihood},
   pages = {77-86},
   title = {Choice of Basis for Laplace Approximation},
   volume = {33},
   year = {1998},
}
@report{,
   author = {M Jordan and J Kleinberg and B Schölkopf},
   title = {Pattern Recognition and Machine Learning},
}
@article{,
   author = {Veronika Ročková and Edward I. George},
   doi = {10.1080/01621459.2016.1260469},
   issn = {0162-1459},
   issue = {521},
   journal = {Journal of the American Statistical Association},
   month = {1},
   pages = {431-444},
   title = {The Spike-and-Slab LASSO},
   volume = {113},
   year = {2018},
}
@article{Bhattacharya2015,
   abstract = {We propose an efficient way to sample from a class of structured multivariate Gaussian distributions which routinely arise as conditional posteriors of model parameters that are assigned a conditionally Gaussian prior. The proposed algorithm only requires matrix operations in the form of matrix multiplications and linear system solutions. We exhibit that the computational complexity of the proposed algorithm grows linearly with the dimension unlike existing algorithms relying on Cholesky factorizations with cubic orders of complexity. The algorithm should be broadly applicable in settings where Gaussian scale mixture priors are used on high dimensional model parameters. We provide an illustration through posterior sampling in a high dimensional regression setting with a horseshoe prior on the vector of regression coefficients.},
   author = {Anirban Bhattacharya and Antik Chakraborty and Bani K. Mallick},
   month = {6},
   title = {Fast sampling with Gaussian scale-mixture priors in high-dimensional regression},
   url = {http://arxiv.org/abs/1506.04778},
   year = {2015},
}
@article{Rue2001,
   abstract = {<p>This paper demonstrates how Gaussian Markov random fields (conditional autoregressions) can be sampled quickly by using numerical techniques for sparse matrices. The algorithm is general and efficient, and expands easily to various forms for conditional simulation and evaluation of normalization constants. We demonstrate its use by constructing efficient block updates in Markov chain Monte Carlo algorithms for disease mapping.</p>},
   author = {Håvard Rue},
   doi = {10.1111/1467-9868.00288},
   issn = {1369-7412},
   issue = {2},
   journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
   month = {7},
   pages = {325-338},
   title = {Fast Sampling of Gaussian Markov Random Fields},
   volume = {63},
   year = {2001},
}
@book_section{Polson2011,
   author = {Nicholas G. Polson and James G. Scott},
   doi = {10.1093/acprof:oso/9780199694587.003.0017},
   journal = {Bayesian Statistics 9},
   month = {10},
   pages = {501-538},
   publisher = {Oxford University Press},
   title = {Shrink Globally, Act Locally: Sparse Bayesian Regularization and Prediction*},
   year = {2011},
}
@article{Makalic2016,
   abstract = {Bayesian penalized regression techniques, such as the Bayesian lasso and the Bayesian horseshoe estimator, have recently received a significant amount of attention in the statistics literature. However, software implementing state-of-the-art Bayesian penalized regression, outside of general purpose Markov chain Monte Carlo platforms such as STAN, is relatively rare. This paper introduces bayesreg, a new toolbox for fitting Bayesian penalized regression models with continuous shrinkage prior densities. The toolbox features Bayesian linear regression with Gaussian or heavy-tailed error models and Bayesian logistic regression with ridge, lasso, horseshoe and horseshoe$+$ estimators. The toolbox is free, open-source and available for use with the MATLAB and R numerical platforms.},
   author = {Enes Makalic and Daniel F. Schmidt},
   month = {11},
   title = {High-Dimensional Bayesian Regularised Regression with the BayesReg Package},
   url = {http://arxiv.org/abs/1611.06649},
   year = {2016},
}
@article{Ishwaran2010,
   abstract = {Weighted generalized ridge regression offers unique advantages in correlated highdimensional problems. Such estimators can be efficiently computed using Bayesian spike and slab models and are effective for prediction. For sparse variable selection, a generalization of the elastic net can be used in tandem with these Bayesian estimates. In this article, we describe the R-software package spikeslab for implementing this new spike and slabprediction and variable selection methodology.},
   author = {Hemant Ishwaran and Udaya B. Kogalur and J. Sunil Rao},
   doi = {10.32614/rj-2010-018},
   issn = {20734859},
   issue = {2},
   journal = {R Journal},
   pages = {68-73},
   publisher = {Technische Universitaet Wien},
   title = {Spikeslab: Prediction and variable selection using spike and slab regression},
   volume = {2},
   year = {2010},
}
@article{Johnstone2004,
   abstract = {An empirical Bayes approach to the estimation of possibly sparse sequences observed in Gaussian white noise is set out and investigated. The prior considered is a mixture of an atom of probability at zero and a heavy-tailed density y, with the mixing weight chosen by marginal maximum likelihood, in the hope of adapting between sparse and dense sequences. If estimation is then carried out using the posterior median, this is a random thresholding procedure. Other thresholding rules employing the same threshold can also be used. Probability bounds on the threshold chosen by the marginal maximum likelihood approach lead to overall risk bounds over classes of signal sequences of length n, allowing for sparsity of various kinds and degrees. The signal classes considered are "nearly black" sequences where only a proportion η is allowed to be nonzero, and sequences with normalized l p norm bounded by η, for η > 0 and 0 > p ≥ 2. Estimation error is measured by mean qth power loss, for 0 < q < 2. For all the classes considered, and for all q in (0, 2], the method achieves the optimal estimation rate as n → ∞ and η → 0 at various rates, and in this sense adapts automatically to the sparseness or otherwise of the underlying signal. In addition the risk is uniformly bounded over all signals. If the posterior mean is used as the estimator, the results still hold for q > 1. Simulations show excellent performance. For appropriately chosen functions y, the method is computationally tractable and software is available. The extension to a modified thresholding method relevant to the estimation of very sparse sequences is also considered. © Institute of Mathematical Statistics, 2004.},
   author = {Iain M. Johnstone and Bernard W. Silverman},
   doi = {10.1214/009053604000000030},
   issn = {00905364},
   issue = {4},
   journal = {Annals of Statistics},
   keywords = {Adaptivity,Empirical Bayes,Sequence estimation,Sparsity,Thresholding},
   month = {8},
   pages = {1594-1649},
   title = {Needles and straw in haystacks: Empirical BAYES estimates of possibly sparse sequences},
   volume = {32},
   year = {2004},
}
@article{Gelling2019,
   abstract = {The rjmcmc package for R implements the post-processing reversible jump Markov chain Monte Carlo (MCMC) algorithm of Barker & Link. MCMC output from each of the models is used to estimate posterior model probabilities and Bayes factors. Automatic differentiation is used to simplify implementation. The package is demonstrated on two examples.},
   author = {Nicholas Gelling and Matthew R. Schofield and Richard J. Barker},
   doi = {10.1111/ANZS.12263},
   issn = {1467842X},
   issue = {2},
   journal = {Australian and New Zealand Journal of Statistics},
   keywords = {Bayes factors,Bayesian multimodel inference,Markov chain Monte Carlo,automatic differentiation},
   pages = {189-212},
   publisher = {John Wiley and Sons Inc},
   title = {R package rjmcmc: reversible jump MCMC using post-processing},
   volume = {61},
   year = {2019},
}
@report{Dobra2009,
   abstract = {We present a Bayesian variable selection procedure that is applicable to genomewide studies involving a combination of clinical, gene expression and genotype information. We use the Mode Oriented Stochastic Search (MOSS) algorithm of Dobra and Massam (2010) to explore regions of high posterior probability for regression models involving discrete covariates and to perform hierarchical log-linear model search to identify the most relevant associations among the resulting subsets of regressors. We illustrate our methodology with simulated data, expression data and SNP data.},
   author = {Adrian Dobra and Laurent Briollais and Hamdi Jarjanazi and Hilmi Ozcelik and Héì Ene Massam},
   keywords = {Bayesian analysis,SNP data,contingency tables,expression data,log-linear mod-els,model selecton,stochastic search,variable selection},
   title = {Applications of the Mode Oriented Stochastic Search (MOSS) Algorithm for Discrete Multi-way Data to Genomewide Studies},
   year = {2009},
}
@article{Dobra2010,
   author = {Adrian Dobra and Héléne Massam},
   doi = {10.1016/j.stamet.2009.04.002},
   issn = {15723127},
   issue = {3},
   journal = {Statistical Methodology},
   month = {5},
   pages = {240-253},
   title = {The mode oriented stochastic search (MOSS) algorithm for log-linear models with conjugate priors},
   volume = {7},
   year = {2010},
}
@report{Erkanli1994,
   abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. This article gives asymptotic expansions for posterior expectations when the mode is on the boundary of the parameter space. The idea, based on the divergence theorem, is to reduce the high-dimensional integrals over the parameters space to surface integrals over the boundary of the parameter space and then apply the usual interior-mode Laplace method to the latter integrals. It is shown that these approximations have second-order accuracy. The method is illustrated with applications to a two-sample binomial problem and a random-effects model.},
   author = {Alaattin Erkanli},
   issue = {425},
   journal = {Source: Journal of the American Statistical Association},
   keywords = {Bayesian inference,Saddle-point approximation,Second-order asymptotics,Tierney-Kadane approximation},
   pages = {250-258},
   title = {Laplace Approximations for Posterior Expectations When the Mode Occurs at the Boundary of the Parameter Space},
   volume = {89},
   year = {1994},
}
@article{Barber2015,
   abstract = {We consider Bayesian variable selection in sparse high-dimensional regression, where the number of covariates $p$ may be large relative to the samples size $n$, but at most a moderate number $q$ of covariates are active. Specifically, we treat generalized linear models. For a single fixed sparse model with well-behaved prior distribution, classical theory proves that the Laplace approximation to the marginal likelihood of the model is accurate for sufficiently large sample size $n$. We extend this theory by giving results on uniform accuracy of the Laplace approximation across all models in a high-dimensional scenario in which $p$ and $q$, and thus also the number of considered models, may increase with $n$. Moreover, we show how this connection between marginal likelihood and Laplace approximation can be used to obtain consistency results for Bayesian approaches to variable selection in high-dimensional regression.},
   author = {Rina Foygel Barber and Mathias Drton and Kean Ming Tan},
   month = {3},
   title = {Laplace Approximation in High-dimensional Bayesian Regression},
   url = {http://arxiv.org/abs/1503.08337},
   year = {2015},
}
@article{,
   abstract = {Despite rapid developments in stochastic search algorithms, the practicality of Bayesian variable selection methods has continued to pose challenges. High-dimensional data are now routinely analyzed, typically with many more covariates than observations. To broaden the applicability of Bayesian variable selection for such high-dimensional linear regression contexts, we propose EMVS, a deterministic alternative to stochastic search based on an EM algorithm which exploits a conjugate mixture prior formulation to quickly find posterior modes. Combining a spike-and-slab regularization diagram for the discovery of active predictor sets with subsequent rigorous evaluation of posteriormodel probabilities, EMVSrapidly identifies promising sparse high posterior probability submodels. External structural information such as likely covariate groupings or network topologies is easily incorporated into the EMVS framework. Deterministic annealing variants are seen to improve the effectiveness of our algorithms by mitigating the posterior multimodality associated with variable selection priors. The usefulness of the EMVS approach is demonstrated on real high-dimensional data, where computational complexity renders stochastic search to be less practical.},
   author = {Veronika Ročková and Edward I. George},
   doi = {10.1080/01621459.2013.869223},
   issn = {1537274X},
   issue = {506},
   journal = {Journal of the American Statistical Association},
   keywords = {Dynamic posterior exploration,High dimensionality,Regularization plots,SSVS,Sparsity},
   pages = {828-846},
   publisher = {American Statistical Association},
   title = {EMVS: The EM approach to Bayesian variable selection},
   volume = {109},
   year = {2014},
}
@report{Tadesse2022,
   author = {Mahlet G Tadesse and Marina Vannucci},
   keywords = {heirarchical,model selection,posterior,prior,shrinkage,spike-and-slab},
   title = {Handbook of Bayesian Variable Selection},
   url = {https://www.crcpress.com/Chapman--HallCRC-},
   year = {2022},
}
@article{Mitchell1988,
   author = {T. J. Mitchell and J. J. Beauchamp},
   doi = {10.2307/2290129},
   issn = {01621459},
   issue = {404},
   journal = {Journal of the American Statistical Association},
   month = {12},
   pages = {1023},
   title = {Bayesian Variable Selection in Linear Regression},
   volume = {83},
   year = {1988},
}
@generic{Ishwaran2005,
   abstract = {Variable selection in the linear regression model takes many apparent faces from both frequentist and Bayesian standpoints. In this paper we introduce a variable selection method referred to as a rescaled spike and slab model. We study the importance of prior hierarchical specifications and draw connections to frequentist generalized ridge regression estimation. Specifically, we study the usefulness of continuous bimodal priors to model hypervariance parameters, and the effect scaling has on the posterior mean through its relationship to penalization. Several model selection strategies, some frequentist and some Bayesian in nature, are developed and studied theoretically. We demonstrate the importance of selective shrinkage for effective variable selection in terms of risk misclassification, and show this is achieved using the posterior from a rescaled spike and slab model. We also show how to verify a procedure's ability to reduce model uncertainty in finite samples using a specialized forward selection strategy. Using this tool, we illustrate the effectiveness of rescaled spike and slab models in reducing model uncertainty. © Institute of Mathematical Statistics, 2005.},
   author = {Hemant Ishwaran and J. Sunil Rao},
   doi = {10.1214/009053604000001147},
   issn = {00905364},
   issue = {2},
   journal = {Annals of Statistics},
   keywords = {Generalized ridge regression,Hypervariance,Model averaging,Model uncertainty,Ordinary least squares,Penalization,Rescaling,Shrinkage,Stochastic variable selection,Zcut},
   month = {4},
   pages = {730-773},
   title = {Spike and slab variable selection: Frequentist and bayesian strategies},
   volume = {33},
   year = {2005},
}
@article{Carvalho2010,
   abstract = {This paper proposes a new approach to sparsity, called the horseshoe estimator, which arises from a prior based on multivariate-normal scale mixtures. We describe the estimator's advantages over existing approaches, including its robustness, adaptivity to different sparsity patterns and analytical tractability. We prove two theorems: one that characterizes the horseshoe estimator's tail robustness and the other that demonstrates a super-efficient rate of convergence to the correct estimate of the sampling density in sparse situations. Finally, using both real and simulated data, we show that the horseshoe estimator corresponds quite closely to the answers obtained by Bayesian model averaging under a point-mass mixture prior. © 2010 Biometrika Trust.},
   author = {Carlos M. Carvalho and Nicholas G. Polson and James G. Scott},
   doi = {10.1093/biomet/asq017},
   issn = {00063444},
   issue = {2},
   journal = {Biometrika},
   keywords = {Normal scale mixture,Ridge regression,Robustness,Shrinkage,Sparsity,Thresholding},
   month = {6},
   pages = {465-480},
   title = {The horseshoe estimator for sparse signals},
   volume = {97},
   year = {2010},
}
@article{Gelling2019,
   abstract = {The rjmcmc package for R implements the post-processing reversible jump Markov chain Monte Carlo (MCMC) algorithm of Barker & Link. MCMC output from each of the models is used to estimate posterior model probabilities and Bayes factors. Automatic differentiation is used to simplify implementation. The package is demonstrated on two examples.},
   author = {Nicholas Gelling and Matthew R. Schofield and Richard J. Barker},
   doi = {10.1111/ANZS.12263},
   issn = {1467842X},
   issue = {2},
   journal = {Australian and New Zealand Journal of Statistics},
   keywords = {Bayes factors,Bayesian multimodel inference,Markov chain Monte Carlo,automatic differentiation},
   pages = {189-212},
   publisher = {John Wiley and Sons Inc},
   title = {R package rjmcmc: reversible jump MCMC using post-processing},
   volume = {61},
   year = {2019},
}
@report{Watkins2010,
   author = {Joseph C. Watkins},
   institution = {University of Arizona},
   title = {Theory of Statistics Contents},
   year = {2010},
}
@report{,
   abstract = {These are class notes for a PhD level course on Bayesian nonparametrics, taught at Columbia University in Fall 2013. This text is a draft.},
   author = {Peter Orbanz},
   title = {Lecture Notes on Bayesian Nonparametrics},
   year = {2014},
}
@article{Moran2019,
   author = {Gemma E. Moran and Veronika Ročková and Edward I. George},
   doi = {10.1214/19-BA1149},
   issn = {1936-0975},
   issue = {4},
   journal = {Bayesian Analysis},
   month = {12},
   title = {Variance Prior Forms for High-Dimensional Bayesian Variable Selection},
   volume = {14},
   year = {2019},
}
@book_section{Bai2021,
   author = {Ray Bai and Veronika Ročková and Edward I. George},
   city = {Boca Raton},
   doi = {10.1201/9781003089018-4},
   journal = {Handbook of Bayesian Variable Selection},
   month = {12},
   pages = {81-108},
   publisher = {Chapman and Hall/CRC},
   title = {Spike-and-Slab Meets LASSO: A Review of the Spike-and-Slab LASSO},
   year = {2021},
}
@article{Kass1995,
   author = {Robert E. Kass and Adrian E. Raftery},
   doi = {10.2307/2291091},
   issn = {01621459},
   issue = {430},
   journal = {Journal of the American Statistical Association},
   month = {6},
   pages = {773},
   title = {Bayes Factors},
   volume = {90},
   year = {1995},
}
@article{Green1995,
   author = {Peter J. Green},
   doi = {10.2307/2337340},
   issn = {00063444},
   issue = {4},
   journal = {Biometrika},
   month = {12},
   pages = {711-732},
   title = {Reversible Jump Markov Chain Monte Carlo Computation and Bayesian Model Determination},
   volume = {82},
   year = {1995},
}
@article{Kim2021,
   abstract = {As a Bayesian criterion for model comparison, Spiegelhalter et al. proposed the deviance information criterion (DIC) which consists of two parts: a classical estimate of fit and an effective number of parameters. This model comparison method is based on generalized linear models, and it may be inappropriate to be used for comparison in the case of mixture of distributions mainly due to the label switching and multimodality issues. For this purpose, Celeux et al. proposed several modified DIC constructions and assessed their behaviors under a mixture of distributions, however they did not fully explore the properties of alternative DICs. Here, we study and provide the properties of DIC3, one of the variations Celeux et al. proposed, and propose our modified DIC to lessen the issue raised by using the dataset twice. We compare our proposed criterion to other model selection criteria based on two numerical examples, the Galaxy dataset and the simulated dataset.},
   author = {Chanmin Kim},
   doi = {10.1080/03610918.2019.1617878},
   issn = {15324141},
   issue = {10},
   journal = {Communications in Statistics: Simulation and Computation},
   keywords = {Bayesian model selection,DIC,Effective number of parameters,Leave-one-out predictive density},
   pages = {2935-2948},
   publisher = {Taylor and Francis Ltd.},
   title = {Deviance information criteria for mixtures of distributions},
   volume = {50},
   year = {2021},
}
@report{Spiegelhalter2014,
   abstract = {The essentials of our paper of 2002 are briefly summarized and compared with other criteria for model comparison. After some comments on the paper's reception and influence, we consider criticisms and proposals for improvement made by us and others. 1. Some background to model comparison Suppose that we have a given set of candidate models, and we would like a criterion to assess which is 'better' in a defined sense. Assume that a model for observed data y postulates a density p.y|θ/ (which may include covariates etc.), and call D.θ/ = −2 log\{p.y|θ/\} the deviance, here considered as a function of θ. Classical model choice uses hypothesis testing for comparing nested models, e.g. the deviance (likelihood ratio) test in generalized linear models. For non-nested models, alternatives include the Akaike information criterion AIC = −2 log\{p.y|ˆθy|ˆ y|ˆθ/\} + 2k wherê θ is the maximum likelihood estimate and k is the number of parameters in the model (dimension of Θ). AIC is built with the aim of favouring models that are likely to make good predictions. Since we generally do not have independent validation data, we can assess which model best predicts the observed data by using the deviance, but if parameters have been estimated we need some penalty for this double use of the data. AIC's penalty of 2k has been shown to be asymptoti-cally equivalent to leave-one-out cross-validation. However, AIC does not work in models with informative prior information, such as hierarchical models, since the prior effectively acts to},
   author = {David J Spiegelhalter and Nicola G Best and Bradley P Carlin},
   journal = {J. R. Statist. Soc. B},
   keywords = {Bayesian,Model comparison,Prediction},
   pages = {485-493},
   title = {The deviance information criterion: 12 years on},
   volume = {76},
   year = {2014},
}
@article{Tanha2017,
   author = {Kiarash Tanha and Neda Mohammadi and Leila Janani},
   doi = {10.14196/mjiri.31.65},
   issn = {10161430},
   issue = {1},
   journal = {Medical Journal of the Islamic Republic of Iran},
   month = {12},
   pages = {377-378},
   title = {P-value: What is and what is not},
   volume = {31},
   year = {2017},
}
@report{Krantz1999,
   abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. A controversy concerning the usefulness of "null" hypothesis tests in scientific inference has continued in articles within psychology since 1960 and has recently come to a head, with serious proposals offered for a test ban or something close to it. This article sketches some of the views of statistical theory and practice among different groups of psychologists, reviews a recent book offering multiple perspectives on null hypothesis tests, and argues that the debate within psychology is a symptom of serious incompleteness in the foundations of statistics.},
   author = {David H Krantz},
   issue = {448},
   journal = {Source: Journal of the American Statistical Association},
   keywords = {Foundations of statistics,Hypothesis tests,Psychometrics},
   pages = {1372-1381},
   title = {The Null Hypothesis Testing Controversy in Psychology},
   volume = {94},
   year = {1999},
}
@book{,
   abstract = {This electronic edition is for non-commercial purposes only. This electronic edition is for non-commercial purposes only.},
   author = {Andrew Gelman and John B Carlin and Hal S Stern and David B Dunson and Aki Vehtari and Donald B Rubin and John Carlin and Hal Stern and Donald Rubin and David Dunson},
   pages = {165-175},
   title = {Bayesian Data Analysis. Third edition},
}
@report{,
   author = {P Bickel and P Diggle and S Fienberg and U Gather and I Olkin and S Zeger},
   title = {Springer Series in Statistics},
   url = {http://www.springer.com/series/692},
}
@book{Kruschke2014,
   abstract = {There is an explosion of interest in Bayesian statistics, primarily because recently created computational methods have finally made Bayesian analysis obtainable to a wide audience. Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan provides an accessible approach to Bayesian data analysis, as material is explained clearly with concrete examples. The book begins with the basics, including essential concepts of probability and random sampling, and gradually progresses to advanced hierarchical modeling methods for realistic data. Included are step-by-step instructions on how to conduct Bayesian data analyses in the popular and free software R and WinBugs. This book is intended for first-year graduate students or advanced undergraduates. It provides a bridge between undergraduate training and modern Bayesian methods for data analysis, which is becoming the accepted research standard. Knowledge of algebra and basic calculus is a prerequisite.},
   author = {John K. Kruschke},
   doi = {10.1016/B978-0-12-405888-0.09999-2},
   isbn = {9780124058880},
   journal = {Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan, Second Edition},
   month = {1},
   pages = {1-759},
   publisher = {Elsevier Science},
   title = {Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan, second edition},
   year = {2014},
}
@report{,
   author = {Richard D Gregory and David W Gibbons and Paul F Donald},
   title = {Bird census and survey techniques},
   url = {www.sustainable-development.gov.uk/indicators/headline/},
}
