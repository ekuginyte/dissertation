@ARTICLE{Meier2008,
title = {The group lasso for logistic regression},
author = {Meier, Lukas and Van De Geer, Sara and Bühlmann, Peter},
year = {2008},
journal = {Journal of the Royal Statistical Society Series B},
volume = {70},
number = {1},
pages = {53-71},
abstract = {Summary. The group lasso is an extension of the lasso to do variable selection on (predefined) groups of variables in linear regression models. The estimates have the attractive property of being invariant under groupwise orthogonal reparameterizations. We extend the group lasso to logistic regression models and present an efficient algorithm, that is especially suitable for high dimensional problems, which can also be applied to generalized linear models to solve the corresponding convex optimization problem. The group lasso estimator for logistic regression is shown to be statistically consistent even if the number of predictors is much larger than sample size but with sparse true underlying structure. We further use a two‐stage procedure which aims for sparser models than the group lasso, leading to improved prediction performance for some cases. Moreover, owing to the two‐stage nature, the estimates can be constructed to be hierarchical. The methods are used on simulated and real data sets about splice site detection in DNA sequences.},
url = {https://EconPapers.repec.org/RePEc:bla:jorssb:v:70:y:2008:i:1:p:53-71},
doi={10.1111/j.1467-9868.2007.00627.x}
}
@misc{Bhadra2016,
      title={Default Bayesian analysis with global-local shrinkage priors}, 
      author={Anindya Bhadra and Jyotishka Datta and Nicholas G. Polson and Brandon T. Willard},
      year={2016},
      eprint={1510.03516},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      doi={10.48550/arXiv.1510.03516}
}
@article{Breheny2011,
author = {Patrick Breheny and Jian Huang},
title = {{Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection}},
volume = {5},
journal = {The Annals of Applied Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {232 -- 253},
abstract = {A number of variable selection methods have been proposed involving nonconvex penalty functions. These methods, which include the smoothly clipped absolute deviation (SCAD) penalty and the minimax concave penalty (MCP), have been demonstrated to have attractive theoretical properties, but model fitting is not a straightforward task, and the resulting solutions may be unstable. Here, we demonstrate the potential of coordinate descent algorithms for fitting these models, establishing theoretical convergence properties and demonstrating that they are significantly faster than competing approaches. In addition, we demonstrate the utility of convexity diagnostics to determine regions of the parameter space in which the objective function is locally convex, even though the penalty is not. Our simulation study and data examples indicate that nonconvex penalties like MCP and SCAD are worthwhile alternatives to the lasso in many applications. In particular, our numerical results suggest that MCP is the preferred approach among the three methods.},
keywords = {Coordinate descent, Lasso, MCP, optimization, penalized regression, SCAD},
year = {2011},
doi = {10.1214/10-AOAS388},
URL = {https://doi.org/10.1214/10-AOAS388}
}
@TechReport{Geweke1996,
  author={John Geweke},
  title={Variable selection and model comparison in regression},
  year=1996,
  month=,
  institution={Federal Reserve Bank of Minneapolis},
  type={Working Papers},
  url={https://ideas.repec.org/p/fip/fedmwp/539.html},
  number={539},
  abstract={In the specification of linear regression models it is common to indicate a list of candidate variables from which a subset enters the model with nonzero coefficients. This paper interprets this specification as a mixed continuous-discrete prior distribution for coefficient values. It then utilizes a Gibbs sampler to construct posterior moments. It is shown how this method can incorporate sign constraints and provide posterior probabilities for all possible subsets of regressors. The methods are illustrated using some standard data sets.},
  keywords={Regression analysis}
}
@article{Hans2009,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/27798870},
 abstract = {The lasso estimate for linear regression corresponds to a posterior mode when independent, double-exponential prior distributions are placed on the regression coefficients. This paper introduces new aspects of the broader Bayesian treatment of lasso regression. A direct characterization of the regression coefficients' posterior distribution is provided, and computation and inference under this characterization is shown to be straightforward. Emphasis is placed on point estimation using the posterior mean, which facilitates prediction of future observations via the posterior predictive distribution. It is shown that the standard lasso prediction method does not necessarily agree with model-based, Bayesian predictions. A new Gibbs sampler for Bayesian lasso regression is introduced.},
 author = {Chris Hans},
 journal = {Biometrika},
 number = {4},
 pages = {835--845},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Bayesian lasso regression},
 urldate = {2023-07-29},
 volume = {96},
 year = {2009}
}
@article{Casella2008,
author = {Trevor Park and George Casella},
title = {The Bayesian Lasso},
journal = {Journal of the American Statistical Association},
volume = {103},
number = {482},
pages = {681-686},
year  = {2008},
publisher = {Taylor & Francis},
doi = {10.1198/016214508000000337}
}
@article{Zou2005,
author = {Zou, Hui and Hastie, Trevor},
year = {2005},
month = {04},
pages = {301 - 320},
title = {Zou H, Hastie T. Regularization and variable selection via the elastic net. J R Statist Soc B. 2005;67(2):301-20},
volume = {67},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
doi = {10.1111/j.1467-9868.2005.00503.x}
}
@article{Tibshirani1996,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {267--288},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression Shrinkage and Selection via the Lasso},
 urldate = {2023-07-26},
 volume = {58},
 year = {1996}
}
@webpage{Li2020,
   author = {Jessica Li},
   title = {When to not use XGBoost?},
   url = {https://www.kaggle.com/discussions/general/196542},
   year = {2020},
}
@article{Guo2020,
   abstract = {<p>Under different degradation conditions, the complexity of natural oscillation of the piston pump will change. Given the difference of the characteristic values of the vibration signal under different degradation states, this paper presents a degradation state recognition method based on improved complete ensemble empirical mode decomposition with adaptive noise (ICEEMDAN) and eXtreme gradient boosting (XGBoost) to improve the accuracy of state recognition. Firstly, ICEEMDAN is proposed to alleviate the mode mixing phenomenon, which decomposes the vibration signal and obtain the intrinsic mode functions (IMFs) with less noise and more physical meaning, and subsequently the optimal IMF is found by using the correlation coefficient method. Then, the time domain, frequency domain, and entropy of the effective IMF are calculated, and the new characteristic values which can represent the degradation state are selected by principal component analysis (PCA) that it realizes dimension reduction. Finally, the above-mentioned characteristic indexes are used as the input of the XGBoost algorithm to achieve the recognition of the degradation state. In this paper, the vibration signals of four different degradation states are generated and analyzed through the piston pump slipper degradation experiment. By comparing the proposed method with different state recognition algorithms, it can be seen that the method based on ICEEMDAN and XGBoost is accurate and efficient, the average accuracy rate can reach more than 99%. Therefore, this method can more accurately describe the degradation state of the piston pump and has a highly practical application value.</p>},
   author = {Rui Guo and Zhiqian Zhao and Tao Wang and Guangheng Liu and Jingyi Zhao and Dianrong Gao},
   doi = {10.3390/app10186593},
   issn = {2076-3417},
   issue = {18},
   journal = {Applied Sciences},
   month = {9},
   pages = {6593},
   title = {Degradation State Recognition of Piston Pump Based on ICEEMDAN and XGBoost},
   volume = {10},
   year = {2020},
}

@article{Scutari2022,
   abstract = {<p>In this paper, we present a general framework for estimating regression models subject to a user-defined level of fairness. We enforce fairness as a model selection step in which we choose the value of a ridge penalty to control the effect of sensitive attributes. We then estimate the parameters of the model conditional on the chosen penalty value. Our proposal is mathematically simple, with a solution that is partly in closed form and produces estimates of the regression coefficients that are intuitive to interpret as a function of the level of fairness. Furthermore, it is easily extended to generalised linear models, kernelised regression models and other penalties, and it can accommodate multiple definitions of fairness. We compare our approach with the regression model from Komiyama et al. (in: Proceedings of machine learning research. 35th international conference on machine learning (ICML), vol 80, pp 2737–2746, 2018), which implements a provably optimal linear regression model and with the fair models from Zafar et al. (J Mach Learn Res 20:1–42, 2019). We evaluate these approaches empirically on six different data sets, and we find that our proposal provides better goodness of fit and better predictive accuracy for the same level of fairness. In addition, we highlight a source of bias in the original experimental evaluation in Komiyama et al. (in: Proceedings of machine learning research. 35th international conference on machine learning (ICML), vol 80, pp 2737–2746, 2018).</p>},
   author = {Marco Scutari and Francesca Panero and Manuel Proissl},
   doi = {10.1007/s11222-022-10143-w},
   issn = {0960-3174},
   issue = {5},
   journal = {Statistics and Computing},
   month = {10},
   pages = {77},
   title = {Achieving fairness with a simple ridge penalty},
   volume = {32},
   year = {2022},
}
@InProceedings{Zliobaite2011,
   abstract = {Historical data used for supervised learning may contain discrimination. We study how to train classifiers on such data, so that they are discrimination free with respect to a given sensitive attribute; e.g., gender. Existing techniques that deal with this problem aim at removing all discrimination and do not take into account that part of the discrimination may be explainable by other attributes, such as, e.g., education level. In this context, we introduce and analyze the issue of conditional non-discrimination in classifier design. We show that some of the differences in decisions across the sensitive groups can be explainable and hence tolerable. We observe that in such cases, the existing discrimination aware techniques will introduce a reverse discrimination, which is undesirable as well. Therefore, we develop local techniques for handling conditional discrimination when one of the attributes is considered to be explanatory. Experimental evaluation demonstrates that the new local techniques remove exactly the bad discrimination, allowing differences in decisions as long as they are explainable. © 2011 IEEE.},
   author = {Indre Žliobaitė and Faisal Kamiran and Toon Calders},
   doi = {10.1109/ICDM.2011.72},
   isbn = {9780769544083},
   issn = {15504786},
   journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
   keywords = {Classification,Discrimination,Independence},
   pages = {992-1001},
   title = {Handling conditional discrimination},
   year = {2011},
}
@InProceedings{Komiyama2018,  
  title = {Nonconvex Optimization for Regression with Fairness Constraints},  
  author = {Komiyama, Junpei and Takeda, Akiko and Honda, Junya and Shimao, Hajime},  
  booktitle =  {Proceedings of the 35th International Conference on Machine Learning},  
  pages =  {2737--2746},  
  year =  {2018},  
  editor =  {Dy, Jennifer and Krause, Andreas},  
  volume =  {80},  
  series =  {Proceedings of Machine Learning Research},  
  month =  {10--15 Jul},  
  publisher =    {PMLR},  
  pdf =  {http://proceedings.mlr.press/v80/komiyama18a/komiyama18a.pdf},  
  url =  {https://proceedings.mlr.press/v80/komiyama18a.html},  
  abstract =  {The unfairness of a regressor is evaluated by measuring the correlation between the estimator and the sensitive attribute (e.g., race, gender, age), and the coefficient of determination (CoD) is a natural extension of the correlation coefficient when more than one sensitive attribute exists. As is well known, there is a trade-off between fairness and accuracy of a regressor, which implies a perfectly fair optimizer does not always yield a useful prediction. Taking this into consideration, we optimize the accuracy of the estimation subject to a user-defined level of fairness. However, a fairness level as a constraint induces a nonconvexity of the feasible region, which disables the use of an off-the-shelf convex optimizer. Despite such nonconvexity, we show an exact solution is available by using tools of global optimization theory. Furthermore, we propose a nonlinear extension of the method by kernel representation. Unlike most of existing fairness-aware machine learning methods, our method allows us to deal with numeric and multiple sensitive attributes.}
}
@report{Kannan2015,
   abstract = {Outlier detection is an important branch in data mining, which is the discovery of data that deviate a lot from other data patterns. Outlier identification can be classified in to formal and informal methods. This paper deals the informal methods also called as labeling methods. Identification of outliers in real time medical data using outlier labeling methods was studied. There are several labeling methods applying in practical situation in the dataset are computed. Finally the estimated results of the outliers are more appropriate way to resolving the large populations.},
   author = {Kaliyaperumal, Senthamarai Kannan and Kuppusamy, Manoj and Arumugam, S},
   issue = {2},
   journal = {International Journal of Statistics and Systems},
   keywords = {Informal methods,Median Absolute Deviation (MAD),Outlier,labeling method},
   pages = {231-238},
   title = {Labeling Methods for Identifying Outliers},
   volume = {10},
   url = {http://www.ripublication.com},
   year = {2015},
}
@article{Vecchi1983,
  doi = {10.1126/science.220.4598.671},
  author = {Kirkpatrick, S  and Gelatt, C D and Vecchi, M P},
  title = {Optimization by Simulated Annealing},
  journal = {Science},
  volume = {220},
  number = {4598},
  pages = {671-680},
  year = {1983},
  doi = {10.1126/science.220.4598.671},
  URL = {https://www.science.org/doi/abs/10.1126/science.220.4598.671},
  eprint = {https://www.science.org/doi/pdf/10.1126/science.220.4598.671},
  abstract = {There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.}
}
@book{Lempers1971,
  title = {Posterior probabilities of alternative linear models: Some theoretical considerations and empirical experiments},
  author = {Lempers, F B},
  year = {1971},
  city = {Rotterdam},
  publisher = {Rotterdam University Press}
}
@article{Barker2013,
  author = {Barker, Richard J and Link, William A},
  title = {Bayesian Multimodel Inference by RJMCMC: A Gibbs Sampling Approach},
  journal = {The American Statistician},
  volume = {67},
  number = {3},
  pages = {150-156},
  year  = {2013},
  doi = {10.1080/00031305.2013.791644},
  publisher = {Taylor & Francis},
  doi = {10.1080/00031305.2013.791644},
  URL = {https://doi.org/10.1080/00031305.2013.791644},
  eprint = {https://doi.org/10.1080/00031305.2013.791644}
}

@article{Carlin1995,
   author = {Bradley, P Carlin and Chib, Siddhartha},
   doi = {10.1111/j.2517-6161.1995.tb02042.x},
   issn = {00359246},
   issue = {3},
   journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
   month = {9},
   pages = {473-484},
   title = {Bayesian Model Choice Via Markov Chain Monte Carlo Methods},
   volume = {57},
   year = {1995}
}
@article{Hoeting1999,
 ISSN = {08834237},
 URL = {http://www.jstor.org/stable/2676803},
 abstract = {Standard statistical practice ignores model uncertainty. Data analysts typically select a model from some class of models and then proceed as if the selected model had generated the data. This approach ignores the uncertainty in model selection, leading to over-confident inferences and decisions that are more risky than one thinks they are. Bayesian model averaging (BMA) provides a coherent mechanism for accounting for this model uncertainty. Several methods for implementing BMA have recently emerged. We discuss these methods and present a number of examples. In these examples, BMA provides improved out-of-sample predictive performance. We also provide a catalogue of currently available BMA software.},
 author = {Hoeting, Jennifer A. and Madigan, David and Raftery, Adrian E. and Volinsky, Chris T.},
 journal = {Statistical Science},
 number = {4},
 pages = {382--401},
 publisher = {Institute of Mathematical Statistics},
 title = {Bayesian Model Averaging: A Tutorial},
 urldate = {2023-07-19},
 volume = {14},
 year = {1999}
}
@article{jeffreys1935, 
  title={Some Tests of Significance, Treated by the Theory of Probability}, 
  volume={31}, 
  DOI={10.1017/S030500410001330X}, 
  number={2}, 
  journal={Mathematical Proceedings of the Cambridge Philosophical Society}, 
  publisher={Cambridge University Press}, 
  author={Jeffreys, Harold}, 
  year={1935}, 
  pages={203–222}
}
@inproceedings{Zafar2017,
   abstract = {Automated data-driven decision making systems are increasingly being used to assist, or even replace humans in many settings. These systems function by learning from historical decisions, often taken by humans. In order to maximize the utility of these systems (or, classifiers), their training involves minimizing the errors (or, misclassifications) over the given historical data. However, it is quite possible that the optimally trained classifier makes decisions for people belonging to different social groups with different misclassification rates (e.g., misclassification rates for females are higher than for males), thereby placing these groups at an unfair disadvantage. To account for and avoid such unfairness, in this paper, we introduce a new notion of unfairness, disparate mistreatment, which is defined in terms of misclassification rates. We then propose intuitive measures of disparate mistreatment for decision boundary-based classifiers, which can be easily incorporated into their formulation as convex-concave constraints. Experiments on synthetic as well as real world datasets show that our methodology is effective at avoiding disparate mistreatment, often at a small cost in terms of accuracy.},
   author = {Zafar, Muhammad Bilal and Valera, Isabel and Rodriguez, Manuel Gomez and Gummadi, Krishna P.},
   doi = {10.1145/3038912.3052660},
   isbn = {9781450349130},
   journal = {26th International World Wide Web Conference, WWW 2017},
   pages = {1171-1180},
   publisher = {International World Wide Web Conferences Steering Committee},
   title = {Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment},
   year = {2017},
}
@misc{misc_communities_and_crime_unnormalized_211,
  author       = {Redmond, Michael},
  title        = {{Communities and Crime Unnormalized}},
  year         = {2011},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5PC8X}
}
@report{Jianqing2010,
   abstract = {High dimensional statistical problems arise from diverse fields of scientific research and technological development. Variable selection plays a pivotal role in contemporary statistical learning and scientific discoveries. The traditional idea of best subset selection methods, which can be regarded as a specific form of penalized likelihood, is computationally too expensive for many modern statistical applications. Other forms of penalized likelihood methods have been successfully developed over the last decade to cope with high dimensionality. They have been widely applied for simultaneously selecting important variables and estimating their effects in high dimensional statistical inference. In this article, we present a brief account of the recent developments of theory, methods, and implementations for high dimensional variable selection. What limits of the dimensionality such methods can handle, what the role of penalty functions is, and what the statistical properties are rapidly drive the advances of the field. The properties of non-concave penalized likelihood and its roles in high dimensional statistical modeling are emphasized. We also review some recent advances in ultra-high dimensional variable selection, with emphasis on independence screening and two-scale methods.},
   author = {Jianqing, Fan and Moore, F L and Jinchi, Lv},
   keywords = {68Q32,LASSO,SCAD,and phrases Variable selection,dimensionality reduction,folded-concave penalty,high dimensionality,model selection,oracle property,penalized least squares,penalized likelihood,secondary 62F12,sure independence screening AMS 2000 subject classifications Primary 62J99,sure screening},
   title = {A Selective Overview of Variable Selection in High Dimensional Feature Space},
   year = {2010},
   journal = {Statistica Sinica}
}
@article{Fan2008,
   abstract = {Summary. Variable selection plays an important role in high dimensional statistical modelling which nowadays appears in many areas and is key to various scientific discoveries. For problems of large scale or dimensionality p, accuracy of estimation and computational cost are two top concerns. Recently, Candes and Tao have proposed the Dantzig selector using L1- regularization and showed that it achieves the ideal risk up to a logarithmic factor log (p). Their innovative procedure and remarkable result are challenged when the dimensionality is ultrahigh as the factor log (p) can be large and their uniform uncertainty principle can fail. Motivated by these concerns, we introduce the concept of sure screening and propose a sure screening method that is based on correlation learning, called sure independence screening, to reduce dimensionality from high to a moderate scale that is below the sample size. In a fairly general asymptotic framework, correlation learning is shown to have the sure screening property for even exponentially growing dimensionality. As a methodological extension, iterative sure independence screening is also proposed to enhance its finite sample performance. With dimension reduced accurately from high to below sample size, variable selection can be improved on both speed and accuracy, and can then be accomplished by a well-developed method such as smoothly clipped absolute deviation, the Dantzig selector, lasso or adaptive lasso. The connections between these penalized least squares methods are also elucidated. © 2008 Royal Statistical Society.},
   author = {Fan, Jianqing and Lv, Jinchi},
   doi = {10.1111/j.1467-9868.2008.00674.x},
   issn = {13697412},
   issue = {5},
   journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
   keywords = {Adaptive lasso,Dantzig selector,Dimensionality reduction,Lasso,Oracle estimator,Smoothly clipped absolute deviation,Sure independence screening,Sure screening,Variable selection},
   month = {11},
   pages = {849-911},
   title = {Sure Independence Screening for Ultrahigh Dimensional Feature Space},
   volume = {70},
   year = {2008},
}
@booksection{Narisetty2020,
   author = {Narisetty, Naveen Naidu},
   doi = {10.1016/bs.host.2019.08.001},
   editor = {Arni S R Srinivasa Rao and C R Rao},
   pages = {207-248},
   publisher = {Elsevier},
   title = {Bayesian model selection for high-dimensional data},
   volume = {43},
   year = {2020},
}
@book{Train2012,
   author = {Train, Kenneth E.},
   doi = {10.1017/CBO9780511805271},
   edition = {2nd Edition},
   isbn = {9780521766555},
   month = {1},
   pages = {284-314},
   publisher = {Cambridge University Press},
   title = {Discrete Choice Methods with Simulation},
   year = {2012},
}
@report{Ewout2019,
   author = {Steyerberg, Ewout W},
   title = {Clinical Prediction Models},
   subtitle = {Statistics for Biology and Health Clinical Prediction Models A Practical Approach to Development, Validation, and Updating Second Edition},
   url = {http://www.springer.com/series/2848},
   year = {2019},
   publisher = {Springer Cham},
   doi = {10.1007/978-3-030-16399-0}
}
@report{Hastie2017,
   author = {Hastie, T and Tibshirani, R and Friedman, J H},
   title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
   publisher = {Springer},
   edition = {2nd Edition},
   city = {New York},
   url = {http://www.springer.com/series/692},
   year = {2017}
}
@book{Kahneman2011,
   author = {Daniel Kahneman},
   city = {New York, NY, US},
   publisher = {Farrar, Straus and Giroux},
   title = {Thinking, Fast and Slow},
   year = {2011},
}
@article{Dakalbab2022,
   abstract = {The security of a community is its topmost priority; hence, governments must take proper actions to reduce the crime rate. Consequently, the application of artificial intelligence (AI) in crime prediction is a significant and well-researched area. This study investigates AI strategies in crime prediction. We conduct a systematic literature review (SLR). Our review evaluates the models from numerous points of view, including the crime analysis type, crimes studied, prediction technique, performance metrics and evaluations, strengths and weaknesses of the proposed method, and limitations and future directions. We review 120 research papers published between 2008 and 2021 that cover AI approaches for crime prediction. We provide 34 crime categories researched by researchers and 23 distinct crime analysis methodologies after analyzing the selected research articles. On the other hand, we identify 64 different machine learning (ML) techniques for crime prediction. In addition, we observe that the most applied approach in crime prediction is the supervised learning approach. Furthermore, we discuss the evaluation and performance metrics, as well as the tools utilized in building the models and their strengths and weaknesses. Crime prediction AI techniques are a promising field of study, and there are several ML models that researchers have applied. Consequently, based upon this review, we provide advice and guidance for researchers working in this area of study.},
   author = {Fatima Dakalbab and Manar Abu Talib and Omnia Abu Waraga and Ali Bou Nassif and Sohail Abbas and Qassim Nasir},
   doi = {10.1016/j.ssaho.2022.100342},
   issn = {25902911},
   issue = {1},
   journal = {Social Sciences & Humanities Open},
   pages = {100342},
   publisher = {Elsevier BV},
   title = {Artificial intelligence & crime prediction: A systematic literature review},
   volume = {6},
   year = {2022},
}
@report{Bergstra2012,
   abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success-they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
   author = {James Bergstra and James Bergstra@umontreal Ca and Yoshua Bengio@umontreal Ca},
   journal = {Journal of Machine Learning Research},
   keywords = {deep learning,global optimization,model selection,neural networks,response surface modeling},
   pages = {281-305},
   title = {Random Search for Hyper-Parameter Optimization Yoshua Bengio},
   volume = {13},
   url = {http://scikit-learn.sourceforge.net.},
   year = {2012},
}
@article{Hasan2020,
   abstract = {Diabetes, also known as chronic illness, is a group of metabolic diseases due to a high level of sugar in the blood over a long period. The risk factor and severity of diabetes can be reduced significantly if the precise early prediction is possible. The robust and accurate prediction of diabetes is highly challenging due to the limited number of labeled data and also the presence of outliers (or missing values) in the diabetes datasets. In this literature, we are proposing a robust framework for diabetes prediction where the outlier rejection, filling the missing values, data standardization, feature selection, K-fold cross-validation, and different Machine Learning (ML) classifiers (k-nearest Neighbour, Decision Trees, Random Forest, AdaBoost, Naive Bayes, and XGBoost) and Multilayer Perceptron (MLP) were employed. The weighted ensembling of different ML models is also proposed, in this literature, to improve the prediction of diabetes where the weights are estimated from the corresponding Area Under ROC Curve (AUC) of the ML model. AUC is chosen as the performance metric, which is then maximized during hyperparameter tuning using the grid search technique. All the experiments, in this literature, were conducted under the same experimental conditions using the Pima Indian Diabetes Dataset. From all the extensive experiments, our proposed ensembling classifier is the best performing classifier with the sensitivity, specificity, false omission rate, diagnostic odds ratio, and AUC as 0.789, 0.934, 0.092, 66.234, and 0.950 respectively which outperforms the state-of-the-art results by 2.00 % in AUC. Our proposed framework for the diabetes prediction outperforms the other methods discussed in the article. It can also provide better results on the same dataset which can lead to better performance in diabetes prediction. Our source code for diabetes prediction is made publicly available.},
   author = {Md Kamrul Hasan and Md Ashraful Alam and Dola Das and Eklas Hossain and Mahmudul Hasan},
   doi = {10.1109/ACCESS.2020.2989857},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {Diabetes prediction,Pima Indian Diabetic dataset,ensembling classifier,machine learning,missing values and outliers,multilayer perceptron},
   pages = {76516-76531},
   publisher = {Institute of Electrical and Electronics Engineers Inc.},
   title = {Diabetes prediction using ensembling of different machine learning classifiers},
   volume = {8},
   year = {2020},
}
@article{Shin2015,
   abstract = {Bayesian model selection procedures based on nonlocal alternative prior densities are extended to ultrahigh dimensional settings and compared to other variable selection procedures using precision-recall curves. Variable selection procedures included in these comparisons include methods based on $g$-priors, reciprocal lasso, adaptive lasso, scad, and minimax concave penalty criteria. The use of precision-recall curves eliminates the sensitivity of our conclusions to the choice of tuning parameters. We find that Bayesian variable selection procedures based on nonlocal priors are competitive to all other procedures in a range of simulation scenarios, and we subsequently explain this favorable performance through a theoretical examination of their consistency properties. When certain regularity conditions apply, we demonstrate that the nonlocal procedures are consistent for linear models even when the number of covariates $p$ increases sub-exponentially with the sample size $n$. A model selection procedure based on Zellner's $g$-prior is also found to be competitive with penalized likelihood methods in identifying the true model, but the posterior distribution on the model space induced by this method is much more dispersed than the posterior distribution induced on the model space by the nonlocal prior methods. We investigate the asymptotic form of the marginal likelihood based on the nonlocal priors and show that it attains a unique term that cannot be derived from the other Bayesian model selection procedures. We also propose a scalable and efficient algorithm called Simplified Shotgun Stochastic Search with Screening (S5) to explore the enormous model space, and we show that S5 dramatically reduces the computing time without losing the capacity to search the interesting region in the model space. The S5 algorithm is available in an \verb R ~package \{\it BayesS5\} on \texttt\{CRAN\}.},
   author = {Minsuk Shin and Anirban Bhattacharya and Valen E. Johnson},
   month = {7},
   title = {Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in Ultrahigh-Dimensional Settings},
   url = {http://arxiv.org/abs/1507.07106},
   year = {2015},
}
@article{Hans2007,
   abstract = {Model search in regression with very large numbers of candidate predictors raises challenges for both model specification and computation, for which standard approaches such as Markov chain Monte Carlo (MCMC) methods are often infeasible or ineffective. We describe a novel shotgun stochastic search (SSS) approach that explores "interesting" regions of the resulting high-dimensional model spaces and quickly identifies regions of high posterior probability over models. We describe algorithmic and modeling aspects, priors over the model space that induce sparsity and parsimony over and above the traditional dimension penalization implicit in Bayesian and likelihood analyses, and parallel computation using cluster computers. We discuss an example from gene expression cancer genomics, comparisons with MCMC and other methods, and theoretical and simulation-based aspects of performance characteristics in large-scale regression model searches. We also provide software implementing the methods. © 2007 American Statistical Association.},
   author = {Chris Hans and Adrian Dobra and Mike West},
   doi = {10.1198/016214507000000121},
   issn = {01621459},
   issue = {478},
   journal = {Journal of the American Statistical Association},
   keywords = {Model averaging,Parallel computing,Regression model uncertainty,Stochastic search,Variable selection},
   month = {6},
   pages = {507-516},
   title = {Shotgun stochastic search for "large p" regression},
   volume = {102},
   year = {2007},
}
@article{Wang2019,
   abstract = {With the rapid development of the Internet, big data has been applied in a large amount of application. However, there are often redundant or irrelevant features in high dimensional data, so feature selection is particularly important. Because the feature subset obtained by a single feature selection method may be biased, an ensemble feature selection method named SA-EFS based on sort aggregation is proposed in this paper, and this method is oriented to classification tasks. For high-dimensional data sets, the results of three feature selection methods, chi-square test, maximum information coefficient and XGBoost, are aggregated by specific strategy. The integration effects of arithmetic mean and geometric mean aggregation strategy on this model are analyzed. In order to evaluate the classification and prediction performance of feature subset, three classifiers with excellent performance, KNN, Random Forest and XGBoost, are tested respectively, and the influence of threshold on classification performance is analyzed. The experimental results show that compared with the single feature selection method, the arithmetic mean aggregation ensemble feature selection can effectively improve the classification accuracy, and the threshold interval setting of 0.1 is a better choice.},
   author = {Jie Wang and Jing Xu and Chengan Zhao and Yan Peng and Hongpeng Wang},
   doi = {10.1080/21642583.2019.1620658},
   issn = {21642583},
   issue = {2},
   journal = {Systems Science and Control Engineering},
   keywords = {Ensemble feature selection,XGBoost,high-dimensional datasets,maximum information coefficient,sort aggregation},
   month = {11},
   pages = {32-39},
   publisher = {Taylor and Francis Ltd.},
   title = {An ensemble feature selection method for high-dimensional data based on sort aggregation},
   volume = {7},
   year = {2019},
}
@article{Chen2016,
   abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
   author = {Tianqi Chen and Carlos Guestrin},
   doi = {10.1145/2939672.2939785},
   month = {3},
   title = {XGBoost: A Scalable Tree Boosting System},
   url = {http://arxiv.org/abs/1603.02754 http://dx.doi.org/10.1145/2939672.2939785},
   year = {2016},
}
@article{Himel2013,
   author = {Himel Mallick Nengjun Yi},
   doi = {10.4172/2155-6180.S1-005},
   issn = {21556180},
   journal = {Journal of Biometrics & Biostatistics},
   title = {Bayesian Methods for High Dimensional Linear Models},
   year = {2013},
}

@article{Casella2009,
   abstract = {It has long been known that for the comparison of pairwise nested models, a decision based on the Bayes factor produces a consistent model selector (in the frequentist sense). Here we go beyond the usual consistency for nested pairwise models, and show that for a wide class of prior distributions, including intrinsic priors, the corresponding Bayesian procedure for variable selection in normal regression is consistent in the entire class of normal linear models. We find that the asymptotics of the Bayes factors for intrinsic priors are equivalent to those of the Schwarz (BIC) criterion. Also, recall that the Jeffreys-Lindley paradox refers to the well-known fact that a point null hypothesis on the normal mean parameter is always accepted when the variance of the conjugate prior goes to infinity. This implies that some limiting forms of proper prior distributions are not necessarily suitable for testing problems. Intrinsic priors are limits of proper prior distributions, and for finite sample sizes they have been proved to behave extremely well for variable selection in regression; a consequence of our results is that for intrinsic priors Lindley's paradox does not arise.},
   author = {George Casella and F. Javier Girón and M. Lina Martinez and Elias Moreno},
   doi = {10.1214/08-AOS606},
   issn = {00905364},
   issue = {3},
   journal = {Annals of Statistics},
   keywords = {Bayes factors,Consistency,Intrinsic priors,Linear models},
   month = {6},
   pages = {1207-1228},
   title = {Consistency of Bayesian Procedures for Variable Selection},
   volume = {37},
   year = {2009},
}
@book{Friston2007,
   abstract = {First edition. In an age where the amount of data collected from brain imaging is increasing constantly, it is of critical importance to analyse those data within an accepted framework to ensure proper integration and comparison of the information collected. This book describes the ideas and procedures that underlie the analysis of signals produced by the brain. The aim is to understand how the brain works, in terms of its functional architecture and dynamics. This book provides the background and methodology for the analysis of all types of brain imaging data, from functional magnetic resonance imaging to magnetoencephalography. Critically, Statistical Parametric Mapping provides a widely accepted conceptual framework which allows treatment of all these different modalities. This rests on an understanding of the brain's functional anatomy and the way that measured signals are caused experimentally. The book takes the reader from the basic concepts underlying the analysis of neuroimaging data to cutting edge approaches that would be difficult to find in any other source. Critically, the material is presented in an incremental way so that the reader can understand the precedents for each new development. This book will be particularly useful to neuroscientists engaged in any form of brain mapping; who have to contend with the real-world problems of data analysis and understanding the techniques they are using. It is primarily a scientific treatment and a didactic introduction to the analysis of brain imaging data. It can be used as both a textbook for students and scientists starting to use the techniques, as well as a reference for practicing neuroscientists. The book also serves as a companion to the software packages that have been developed for brain imaging data analysis. * An essential reference and companion for users of the SPM software * Provides a complete description of the concepts and procedures entailed by the analysis of brain images * Offers full didactic treatment of the basic mathematics behind the analysis of brain imaging data * Stands as a compendium of all the advances in neuroimaging data analysis over the past decade * Adopts an easy to understand and incremental approach that takes the reader from basic statistics to state of the art approaches such as Variational Bayes * Structured treatment of data analysis issues that links different modalities and models * Includes a series of appendices and tutorial-style chapters that makes even the most sophisticated approaches accessible. INTRODUCTION -- A short history of SPM. -- Statistical parametric mapping. -- Modelling brain responses. -- SECTION 1: COMPUTATIONAL ANATOMY -- Rigid-body Registration. -- Nonlinear Registration. -- Segmentation. -- Voxel-based Morphometry. -- SECTION 2: GENERAL LINEAR MODELS -- The General Linear Model. -- Contrasts & Classical Inference. -- Covariance Components. -- Hierarchical models. -- Random Effects Analysis. -- Analysis of variance. -- Convolution models for fMRI. -- Efficient Experimental Design for fMRI. -- Hierarchical models for EEG/MEG. -- SECTION 3: CLASSICAL INFERENCE -- Parametric procedures for imaging. -- Random Field Theory & inference. -- Topological Inference. -- False discovery rate procedures. -- Non-parametric procedures. -- SECTION 4: BAYESIAN INFERENCE -- Empirical Bayes & hierarchical models. -- Posterior probability maps. -- Variational Bayes. -- Spatiotemporal models for fMRI. -- Spatiotemporal models for EEG. -- SECTION 5: BIOPHYSICAL MODELS -- Forward models for fMRI. -- Forward models for EEG and MEG. -- Bayesian inversion of EEG models. -- Bayesian inversion for induced responses. -- Neuronal models of ensemble dynamics. -- Neuronal models of energetics. -- Neuronal models of EEG and MEG. -- Bayesian inversion of dynamic models -- Bayesian model selection & averaging. -- SECTION 6: CONNECTIVITY -- Functional integration. -- Functional Connectivity. -- Effective Connectivity. -- Nonlinear coupling and Kernels. -- Multivariate autoregressive models. -- Dynamic Causal Models for fMRI. -- Dynamic Causal Models for EEG. -- Dynamic Causal Models & Bayesian selection. -- APPENDICES -- Linear models and inference. -- Dynamical systems. -- Expectation maximisation. -- Variational Bayes under the Laplace approximation. -- Kalman Filtering. -- Random Field Theory.},
   author = {Friston, Karl J and Ashburner, John and Kiebel, Stefan and Nichols, Thomas and Penny, William D.},
   isbn = {9780123725608},
   pages = {647},
   publisher = {Elsevier/Academic Press},
   title = {Statistical Parametric Mapping: The Analysis of Funtional Brain Images},
   year = {2007},
}
@report{Mackay1998,
   abstract = {Maximum a posteriori optimization of parameters and the Laplace approximation for the marginal likelihood are both basis-dependent methods. This note compares two choices of basis for models parameterized by probabilities, showing that it is possible to improve on the traditional choice, the probability simplex, by transforming to the 'softmax' basis.},
   author = {David J C Mackay},
   journal = {Machine Learning},
   keywords = {Bayes factor,Bayesian inference,graphical models,hidden Markov models,latent variable models,marginal likelihood},
   pages = {77-86},
   title = {Choice of Basis for Laplace Approximation},
   volume = {33},
   year = {1998},
}
@article{Rockova2018,
   author = {Veronika Ročková and Edward I. George},
   doi = {10.1080/01621459.2016.1260469},
   issn = {0162-1459},
   issue = {521},
   journal = {Journal of the American Statistical Association},
   month = {1},
   pages = {431-444},
   title = {The Spike-and-Slab LASSO},
   volume = {113},
   year = {2018},
}
@article{Bhattacharya2016,
   abstract = {We propose an efficient way to sample from a class of structured multivariate Gaussian distributions which routinely arise as conditional posteriors of model parameters that are assigned a conditionally Gaussian prior. The proposed algorithm only requires matrix operations in the form of matrix multiplications and linear system solutions. We exhibit that the computational complexity of the proposed algorithm grows linearly with the dimension unlike existing algorithms relying on Cholesky factorizations with cubic orders of complexity. The algorithm should be broadly applicable in settings where Gaussian scale mixture priors are used on high dimensional model parameters. We provide an illustration through posterior sampling in a high dimensional regression setting with a horseshoe prior on the vector of regression coefficients.},
   author = {Anirban Bhattacharya and Antik Chakraborty and Bani K. Mallick},
   month = {6},
   title = {Fast sampling with Gaussian scale-mixture priors in high-dimensional regression},
   url = {http://arxiv.org/abs/1506.04778},
   year = {2016},
}
@article{Rue2001,
   abstract = {<p>This paper demonstrates how Gaussian Markov random fields (conditional autoregressions) can be sampled quickly by using numerical techniques for sparse matrices. The algorithm is general and efficient, and expands easily to various forms for conditional simulation and evaluation of normalization constants. We demonstrate its use by constructing efficient block updates in Markov chain Monte Carlo algorithms for disease mapping.</p>},
   author = {Håvard Rue},
   doi = {10.1111/1467-9868.00288},
   issn = {1369-7412},
   issue = {2},
   journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
   month = {7},
   pages = {325-338},
   title = {Fast Sampling of Gaussian Markov Random Fields},
   volume = {63},
   year = {2001},
}
@booksection{Polson2010,
   author = {Nicholas G. Polson and James G. Scott},
   doi = {10.1093/acprof:oso/9780199694587.003.0017},
   journal = {Bayesian Statistics 9},
   month = {10},
   pages = {501-538},
   publisher = {Oxford University Press},
   title = {Shrink Globally, Act Locally: Sparse Bayesian Regularization and Prediction*},
   year = {2010},
}
@article{Makalic2016,
   abstract = {Bayesian penalized regression techniques, such as the Bayesian lasso and the Bayesian horseshoe estimator, have recently received a significant amount of attention in the statistics literature. However, software implementing state-of-the-art Bayesian penalized regression, outside of general purpose Markov chain Monte Carlo platforms such as STAN, is relatively rare. This paper introduces bayesreg, a new toolbox for fitting Bayesian penalized regression models with continuous shrinkage prior densities. The toolbox features Bayesian linear regression with Gaussian or heavy-tailed error models and Bayesian logistic regression with ridge, lasso, horseshoe and horseshoe$+$ estimators. The toolbox is free, open-source and available for use with the MATLAB and R numerical platforms.},
   author = {Enes Makalic and Daniel F. Schmidt},
   month = {11},
   title = {High-Dimensional Bayesian Regularised Regression with the BayesReg Package},
   arxiv = {http://arxiv.org/abs/1611.06649},
   year = {2016},
}
@article{Ishwaran2010,
   abstract = {Weighted generalized ridge regression offers unique advantages in correlated highdimensional problems. Such estimators can be efficiently computed using Bayesian spike and slab models and are effective for prediction. For sparse variable selection, a generalization of the elastic net can be used in tandem with these Bayesian estimates. In this article, we describe the R-software package spikeslab for implementing this new spike and slabprediction and variable selection methodology.},
   author = {Hemant Ishwaran and Udaya B. Kogalur and J. Sunil Rao},
   doi = {10.32614/rj-2010-018},
   issn = {20734859},
   issue = {2},
   journal = {R Journal},
   pages = {68-73},
   publisher = {Technische Universitaet Wien},
   title = {Spikeslab: Prediction and variable selection using spike and slab regression},
   volume = {2},
   year = {2010},
}
@article{Johnstone2004,
   abstract = {An empirical Bayes approach to the estimation of possibly sparse sequences observed in Gaussian white noise is set out and investigated. The prior considered is a mixture of an atom of probability at zero and a heavy-tailed density y, with the mixing weight chosen by marginal maximum likelihood, in the hope of adapting between sparse and dense sequences. If estimation is then carried out using the posterior median, this is a random thresholding procedure. Other thresholding rules employing the same threshold can also be used. Probability bounds on the threshold chosen by the marginal maximum likelihood approach lead to overall risk bounds over classes of signal sequences of length n, allowing for sparsity of various kinds and degrees. The signal classes considered are "nearly black" sequences where only a proportion η is allowed to be nonzero, and sequences with normalized l p norm bounded by η, for η > 0 and 0 > p ≥ 2. Estimation error is measured by mean qth power loss, for 0 < q < 2. For all the classes considered, and for all q in (0, 2], the method achieves the optimal estimation rate as n → ∞ and η → 0 at various rates, and in this sense adapts automatically to the sparseness or otherwise of the underlying signal. In addition the risk is uniformly bounded over all signals. If the posterior mean is used as the estimator, the results still hold for q > 1. Simulations show excellent performance. For appropriately chosen functions y, the method is computationally tractable and software is available. The extension to a modified thresholding method relevant to the estimation of very sparse sequences is also considered. © Institute of Mathematical Statistics, 2004.},
   author = {Iain M. Johnstone and Bernard W. Silverman},
   doi = {10.1214/009053604000000030},
   issn = {00905364},
   issue = {4},
   journal = {Annals of Statistics},
   keywords = {Adaptivity,Empirical Bayes,Sequence estimation,Sparsity,Thresholding},
   month = {8},
   pages = {1594-1649},
   title = {Needles and straw in haystacks: Empirical BAYES estimates of possibly sparse sequences},
   volume = {32},
   year = {2004},
}
@article{Gelling2019,
   abstract = {The rjmcmc package for R implements the post-processing reversible jump Markov chain Monte Carlo (MCMC) algorithm of Barker & Link. MCMC output from each of the models is used to estimate posterior model probabilities and Bayes factors. Automatic differentiation is used to simplify implementation. The package is demonstrated on two examples.},
   author = {Nicholas Gelling and Matthew R. Schofield and Richard J. Barker},
   doi = {10.1111/ANZS.12263},
   issn = {1467842X},
   issue = {2},
   journal = {Australian and New Zealand Journal of Statistics},
   keywords = {Bayes factors,Bayesian multimodel inference,Markov chain Monte Carlo,automatic differentiation},
   pages = {189-212},
   publisher = {John Wiley and Sons Inc},
   title = {R package rjmcmc: reversible jump MCMC using post-processing},
   volume = {61},
   year = {2019},
}
@report{Dobra2009,
   abstract = {We present a Bayesian variable selection procedure that is applicable to genomewide studies involving a combination of clinical, gene expression and genotype information. We use the Mode Oriented Stochastic Search (MOSS) algorithm of Dobra and Massam (2010) to explore regions of high posterior probability for regression models involving discrete covariates and to perform hierarchical log-linear model search to identify the most relevant associations among the resulting subsets of regressors. We illustrate our methodology with simulated data, expression data and SNP data.},
   author = {Adrian Dobra and Laurent Briollais and Hamdi Jarjanazi and Hilmi Ozcelik and Héì Ene Massam},
   keywords = {Bayesian analysis,SNP data,contingency tables,expression data,log-linear mod-els,model selecton,stochastic search,variable selection},
   title = {Applications of the Mode Oriented Stochastic Search (MOSS) Algorithm for Discrete Multi-way Data to Genomewide Studies},
   year = {2009},
}
@article{Dobra2010,
   author = {Adrian Dobra and Héléne Massam},
   doi = {10.1016/j.stamet.2009.04.002},
   issn = {15723127},
   issue = {3},
   journal = {Statistical Methodology},
   month = {5},
   pages = {240-253},
   title = {The mode oriented stochastic search (MOSS) algorithm for log-linear models with conjugate priors},
   volume = {7},
   year = {2010},
}
@report{Erkanli1994,
   abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. This article gives asymptotic expansions for posterior expectations when the mode is on the boundary of the parameter space. The idea, based on the divergence theorem, is to reduce the high-dimensional integrals over the parameters space to surface integrals over the boundary of the parameter space and then apply the usual interior-mode Laplace method to the latter integrals. It is shown that these approximations have second-order accuracy. The method is illustrated with applications to a two-sample binomial problem and a random-effects model.},
   author = {Alaattin Erkanli},
   issue = {425},
   journal = {Source: Journal of the American Statistical Association},
   keywords = {Bayesian inference,Saddle-point approximation,Second-order asymptotics,Tierney-Kadane approximation},
   pages = {250-258},
   title = {Laplace Approximations for Posterior Expectations When the Mode Occurs at the Boundary of the Parameter Space},
   volume = {89},
   year = {1994},
}
@article{Barber2015,
   abstract = {We consider Bayesian variable selection in sparse high-dimensional regression, where the number of covariates $p$ may be large relative to the samples size $n$, but at most a moderate number $q$ of covariates are active. Specifically, we treat generalized linear models. For a single fixed sparse model with well-behaved prior distribution, classical theory proves that the Laplace approximation to the marginal likelihood of the model is accurate for sufficiently large sample size $n$. We extend this theory by giving results on uniform accuracy of the Laplace approximation across all models in a high-dimensional scenario in which $p$ and $q$, and thus also the number of considered models, may increase with $n$. Moreover, we show how this connection between marginal likelihood and Laplace approximation can be used to obtain consistency results for Bayesian approaches to variable selection in high-dimensional regression.},
   author = {Rina Foygel Barber and Mathias Drton and Kean Ming Tan},
   month = {3},
   title = {Laplace Approximation in High-dimensional Bayesian Regression},
   url = {http://arxiv.org/abs/1503.08337},
   year = {2015},
}
@article{Rockova2014,
   abstract = {Despite rapid developments in stochastic search algorithms, the practicality of Bayesian variable selection methods has continued to pose challenges. High-dimensional data are now routinely analyzed, typically with many more covariates than observations. To broaden the applicability of Bayesian variable selection for such high-dimensional linear regression contexts, we propose EMVS, a deterministic alternative to stochastic search based on an EM algorithm which exploits a conjugate mixture prior formulation to quickly find posterior modes. Combining a spike-and-slab regularization diagram for the discovery of active predictor sets with subsequent rigorous evaluation of posteriormodel probabilities, EMVSrapidly identifies promising sparse high posterior probability submodels. External structural information such as likely covariate groupings or network topologies is easily incorporated into the EMVS framework. Deterministic annealing variants are seen to improve the effectiveness of our algorithms by mitigating the posterior multimodality associated with variable selection priors. The usefulness of the EMVS approach is demonstrated on real high-dimensional data, where computational complexity renders stochastic search to be less practical.},
   author = {Veronika Ročková and Edward I. George},
   doi = {10.1080/01621459.2013.869223},
   issn = {1537274X},
   issue = {506},
   journal = {Journal of the American Statistical Association},
   keywords = {Dynamic posterior exploration,High dimensionality,Regularization plots,SSVS,Sparsity},
   pages = {828-846},
   publisher = {American Statistical Association},
   title = {EMVS: The EM approach to Bayesian variable selection},
   volume = {109},
   year = {2014},
}
@report{Tadesse2022,
   author = {Mahlet G Tadesse and Marina Vannucci},
   keywords = {heirarchical,model selection,posterior,prior,shrinkage,spike-and-slab},
   title = {Handbook of Bayesian Variable Selection},
   publisher = {Chapman & Hall},
   doi = {10.1080/00031305.2013.791644},
   year = {2022},
   
}
@article{Mitchell1988,
   author = {T. J. Mitchell and J. J. Beauchamp},
   doi = {10.2307/2290129},
   issn = {01621459},
   issue = {404},
   journal = {Journal of the American Statistical Association},
   month = {12},
   pages = {1023},
   title = {Bayesian Variable Selection in Linear Regression},
   volume = {83},
   year = {1988},
}
@generic{Ishwaran2005,
   abstract = {Variable selection in the linear regression model takes many apparent faces from both frequentist and Bayesian standpoints. In this paper we introduce a variable selection method referred to as a rescaled spike and slab model. We study the importance of prior hierarchical specifications and draw connections to frequentist generalized ridge regression estimation. Specifically, we study the usefulness of continuous bimodal priors to model hypervariance parameters, and the effect scaling has on the posterior mean through its relationship to penalization. Several model selection strategies, some frequentist and some Bayesian in nature, are developed and studied theoretically. We demonstrate the importance of selective shrinkage for effective variable selection in terms of risk misclassification, and show this is achieved using the posterior from a rescaled spike and slab model. We also show how to verify a procedure's ability to reduce model uncertainty in finite samples using a specialized forward selection strategy. Using this tool, we illustrate the effectiveness of rescaled spike and slab models in reducing model uncertainty. © Institute of Mathematical Statistics, 2005.},
   author = {Hemant Ishwaran and J. Sunil Rao},
   doi = {10.1214/009053604000001147},
   issn = {00905364},
   issue = {2},
   journal = {Annals of Statistics},
   keywords = {Generalized ridge regression,Hypervariance,Model averaging,Model uncertainty,Ordinary least squares,Penalization,Rescaling,Shrinkage,Stochastic variable selection,Zcut},
   month = {4},
   pages = {730-773},
   title = {Spike and Slab Variable Selection: Frequentist and Bayesian Strategies},
   volume = {33},
   year = {2005},
}
@article{Carvalho2010,
   abstract = {This paper proposes a new approach to sparsity, called the horseshoe estimator, which arises from a prior based on multivariate-normal scale mixtures. We describe the estimator's advantages over existing approaches, including its robustness, adaptivity to different sparsity patterns and analytical tractability. We prove two theorems: one that characterizes the horseshoe estimator's tail robustness and the other that demonstrates a super-efficient rate of convergence to the correct estimate of the sampling density in sparse situations. Finally, using both real and simulated data, we show that the horseshoe estimator corresponds quite closely to the answers obtained by Bayesian model averaging under a point-mass mixture prior. © 2010 Biometrika Trust.},
   author = {Carlos M. Carvalho and Nicholas G. Polson and James G. Scott},
   doi = {10.1093/biomet/asq017},
   issn = {00063444},
   issue = {2},
   journal = {Biometrika},
   keywords = {Normal scale mixture,Ridge regression,Robustness,Shrinkage,Sparsity,Thresholding},
   month = {6},
   pages = {465-480},
   title = {The horseshoe estimator for sparse signals},
   volume = {97},
   year = {2010},
}
@article{Gelling2019,
   abstract = {The rjmcmc package for R implements the post-processing reversible jump Markov chain Monte Carlo (MCMC) algorithm of Barker & Link. MCMC output from each of the models is used to estimate posterior model probabilities and Bayes factors. Automatic differentiation is used to simplify implementation. The package is demonstrated on two examples.},
   author = {Nicholas Gelling and Matthew R. Schofield and Richard J. Barker},
   doi = {10.1111/ANZS.12263},
   issn = {1467842X},
   issue = {2},
   journal = {Australian and New Zealand Journal of Statistics},
   keywords = {Bayes factors,Bayesian multimodel inference,Markov chain Monte Carlo,automatic differentiation},
   pages = {189-212},
   publisher = {John Wiley and Sons Inc},
   title = {R Package rjmcmc: Reversible Mump MCMC Using Post-Processing},
   volume = {61},
   year = {2019},
}
@report{Watkins2010,
   author = {Joseph C. Watkins},
   institution = {University of Arizona},
   title = {Theory of Statistics Contents},
   year = {2010},
}
@report{Orbanz2014,
   abstract = {These are class notes for a PhD level course on Bayesian nonparametrics, taught at Columbia University in Fall 2013. This text is a draft.},
   author = {Peter Orbanz},
   title = {Lecture Notes on Bayesian Nonparametrics},
   year = {2014},
}
@article{Moran2019,
   author = {Gemma E. Moran and Veronika Ročková and Edward I. George},
   doi = {10.1214/19-BA1149},
   issn = {1936-0975},
   issue = {4},
   journal = {Bayesian Analysis},
   month = {12},
   title = {Variance Prior Forms for High-Dimensional Bayesian Variable Selection},
   volume = {14},
   year = {2019},
}
@booksection{Bai2021,
   author = {Ray Bai and Veronika Ročková and Edward I. George},
   city = {Boca Raton},
   doi = {10.1201/9781003089018-4},
   journal = {Handbook of Bayesian Variable Selection},
   month = {12},
   pages = {81-108},
   publisher = {Chapman and Hall/CRC},
   title = {Spike-and-Slab Meets LASSO: A Review of the Spike-and-Slab LASSO},
   year = {2021},
}
@article{Kass1995,
   author = {Robert E. Kass and Adrian E. Raftery},
   doi = {10.2307/2291091},
   issn = {01621459},
   issue = {430},
   journal = {Journal of the American Statistical Association},
   month = {6},
   pages = {773},
   title = {Bayes Factors},
   volume = {90},
   year = {1995},
}
@article{Green1995,
   author = {Peter J. Green},
   doi = {10.2307/2337340},
   issn = {00063444},
   issue = {4},
   journal = {Biometrika},
   month = {12},
   pages = {711-732},
   title = {Reversible Jump Markov Chain Monte Carlo Computation and Bayesian Model Determination},
   volume = {82},
   year = {1995},
}
@article{Kim2021,
   abstract = {As a Bayesian criterion for model comparison, Spiegelhalter et al. proposed the deviance information criterion (DIC) which consists of two parts: a classical estimate of fit and an effective number of parameters. This model comparison method is based on generalized linear models, and it may be inappropriate to be used for comparison in the case of mixture of distributions mainly due to the label switching and multimodality issues. For this purpose, Celeux et al. proposed several modified DIC constructions and assessed their behaviors under a mixture of distributions, however they did not fully explore the properties of alternative DICs. Here, we study and provide the properties of DIC3, one of the variations Celeux et al. proposed, and propose our modified DIC to lessen the issue raised by using the dataset twice. We compare our proposed criterion to other model selection criteria based on two numerical examples, the Galaxy dataset and the simulated dataset.},
   author = {Chanmin Kim},
   doi = {10.1080/03610918.2019.1617878},
   issn = {15324141},
   issue = {10},
   journal = {Communications in Statistics: Simulation and Computation},
   keywords = {Bayesian model selection,DIC,Effective number of parameters,Leave-one-out predictive density},
   pages = {2935-2948},
   publisher = {Taylor and Francis Ltd.},
   title = {Deviance information criteria for mixtures of distributions},
   volume = {50},
   year = {2021},
}
@report{Spiegelhalter2014,
   abstract = {The essentials of our paper of 2002 are briefly summarized and compared with other criteria for model comparison. After some comments on the paper's reception and influence, we consider criticisms and proposals for improvement made by us and others. 1. Some background to model comparison Suppose that we have a given set of candidate models, and we would like a criterion to assess which is 'better' in a defined sense. Assume that a model for observed data y postulates a density p.y|θ/ (which may include covariates etc.), and call D.θ/ = −2 log\{p.y|θ/\} the deviance, here considered as a function of θ. Classical model choice uses hypothesis testing for comparing nested models, e.g. the deviance (likelihood ratio) test in generalized linear models. For non-nested models, alternatives include the Akaike information criterion AIC = −2 log\{p.y|ˆθy|ˆ y|ˆθ/\} + 2k wherê θ is the maximum likelihood estimate and k is the number of parameters in the model (dimension of Θ). AIC is built with the aim of favouring models that are likely to make good predictions. Since we generally do not have independent validation data, we can assess which model best predicts the observed data by using the deviance, but if parameters have been estimated we need some penalty for this double use of the data. AIC's penalty of 2k has been shown to be asymptoti-cally equivalent to leave-one-out cross-validation. However, AIC does not work in models with informative prior information, such as hierarchical models, since the prior effectively acts to},
   author = {Spiegelhalter, David J and Best, Nicola G and Carlin, Bradley P and Linde, Angelika van der},
   journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
   keywords = {Bayesian,Model comparison,Prediction},
   pages = {485-493},
   title = {The deviance information criterion: 12 years on},
   volume = {76. No. 3},
   year = {2014},
}
@article{Tanha2017,
   author = {Kiarash Tanha and Neda Mohammadi and Leila Janani},
   doi = {10.14196/mjiri.31.65},
   issn = {10161430},
   issue = {1},
   journal = {Medical Journal of the Islamic Republic of Iran},
   month = {12},
   pages = {377-378},
   title = {P-value: What is and what is not},
   volume = {31},
   year = {2017},
}
@report{Krantz1999,
   abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. A controversy concerning the usefulness of "null" hypothesis tests in scientific inference has continued in articles within psychology since 1960 and has recently come to a head, with serious proposals offered for a test ban or something close to it. This article sketches some of the views of statistical theory and practice among different groups of psychologists, reviews a recent book offering multiple perspectives on null hypothesis tests, and argues that the debate within psychology is a symptom of serious incompleteness in the foundations of statistics.},
   author = {David H Krantz},
   issue = {448},
   journal = {Source: Journal of the American Statistical Association},
   keywords = {Foundations of statistics,Hypothesis tests,Psychometrics},
   pages = {1372-1381},
   title = {The Null Hypothesis Testing Controversy in Psychology},
   volume = {94},
   year = {1999},
}
@book{Gelman2020,
   abstract = {This electronic edition is for non-commercial purposes only. This electronic edition is for non-commercial purposes only.},
   author = {Andrew Gelman and John B Carlin and Hal S Stern and David B Dunson and Aki Vehtari and Donald B Rubin},
   pages = {165-175},
   title = {Bayesian Data Analysis. Third edition},
   year = {2020}
}
@book{Kruschke2014,
   abstract = {There is an explosion of interest in Bayesian statistics, primarily because recently created computational methods have finally made Bayesian analysis obtainable to a wide audience. Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan provides an accessible approach to Bayesian data analysis, as material is explained clearly with concrete examples. The book begins with the basics, including essential concepts of probability and random sampling, and gradually progresses to advanced hierarchical modeling methods for realistic data. Included are step-by-step instructions on how to conduct Bayesian data analyses in the popular and free software R and WinBugs. This book is intended for first-year graduate students or advanced undergraduates. It provides a bridge between undergraduate training and modern Bayesian methods for data analysis, which is becoming the accepted research standard. Knowledge of algebra and basic calculus is a prerequisite.},
   author = {John K. Kruschke},
   doi = {10.1016/B978-0-12-405888-0.09999-2},
   isbn = {9780124058880},
   journal = {Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan, Second Edition},
   month = {1},
   pages = {1-759},
   publisher = {Elsevier Science},
   title = {Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan, second edition},
   year = {2014},
}
